{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhjT6P48A_xj"
      },
      "outputs": [],
      "source": [
        "# بسم الله الرحمن الرحیم"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BaseLine for HDvoicer"
      ],
      "metadata": {
        "id": "K0rjERiYBLNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTCshuMMBPZU",
        "outputId": "f9f1c44b-a657-46a0-ca1e-9eea5b20b240"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MRH: You may need to downconvert and install some libraries to be compatible with the code**"
      ],
      "metadata": {
        "id": "vdjb1MWHCAm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"librosa==0.9.1\""
      ],
      "metadata": {
        "id": "BZyERmkeCQAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "OFlkJhAcCYSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MRH: The Following part is used one time during feature Extraction, So, after one round you can skip this part.\n",
        "Also, the addresses shouls be modified. since my address correrspond to my google drive.**"
      ],
      "metadata": {
        "id": "MwpO_J5KBVAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess/tacotron/hyperparams.py\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "#/usr/bin/python2\n",
        "'''\n",
        "By kyubyong park. kbpark.linguist@gmail.com.\n",
        "https://www.github.com/kyubyong/tacotron\n",
        "'''\n",
        "class Hyperparams:\n",
        "    '''Hyper parameters'''\n",
        "\n",
        "    top_db = 15\n",
        "\n",
        "    # signal processing\n",
        "    sr = 22050 # Sample rate.\n",
        "    # n_fft = 2048 # fft points (samples)\n",
        "    n_fft = 1024 # fft points (samples)\n",
        "    # frame_shift = 0.0125 # seconds\n",
        "    # frame_length = 0.05 # seconds\n",
        "    # hop_length = int(sr*frame_shift) # samples.\n",
        "    hop_length = 256 # samples.\n",
        "    # win_length = int(sr*frame_length) # samples.\n",
        "    win_length = 1024 # samples.\n",
        "    n_mels = 80 # Number of Mel banks to generate\n",
        "    power = 1.2 # Exponent for amplifying the predicted magnitude\n",
        "    n_iter = 100 # Number of inversion iterations\n",
        "    preemphasis = .97 # or None\n",
        "    max_db = 100\n",
        "    ref_db = 20"
      ],
      "metadata": {
        "id": "LBJlO2xKBugH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to test\n",
        "hp = Hyperparams()\n",
        "print(hp.sr)\n",
        "print(hp.n_mels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kNiS7fdCez9",
        "outputId": "e5a7ac32-c83f-4e1c-ceaa-3c49b95c18e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22050\n",
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adaptive_voice_conversion/preprocess/make_datasets_vctk.py\n",
        "\n",
        "#\n",
        "\n",
        "import pickle\n",
        "import librosa\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import numpy as np\n",
        "import json\n",
        "# from tacotron.utils import get_spectrograms\n",
        "\n",
        "def read_speaker_info(speaker_info_path):\n",
        "    speaker_ids = []\n",
        "    with open(speaker_info_path, 'r') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            speaker_id = line.strip().split()[0]\n",
        "            speaker_ids.append(speaker_id)\n",
        "    return speaker_ids\n",
        "\n",
        "\n",
        "##### MRH: This is Very Important... using *.flac or *.wav\n",
        "\n",
        "def read_filenames(root_dir):\n",
        "    speaker2filenames = defaultdict(lambda : [])\n",
        "    for path in sorted(glob.glob(os.path.join(root_dir, '*/p*_mic2.flac'))):\n",
        "        filename = os.path.basename(path)\n",
        "        match = re.match(r'p(\\d+)_(\\d+)_mic2\\.flac', filename)\n",
        "\n",
        "        if match:\n",
        "            # THIS IS THE CRITICAL CHANGE: Prepend 'p' to the captured digits\n",
        "            speaker_id = 'p' + match.groups()[0]  # match.groups()[0] gets '225', '226', etc.\n",
        "            # utt_id = match.groups()[1] # You don't use this, but it would be the second group\n",
        "\n",
        "            speaker2filenames[speaker_id].append(path)\n",
        "        else:\n",
        "            print(f\"Warning: Filename '{filename}' matched glob but not regex. Skipping. Path: {path}\")\n",
        "\n",
        "    return speaker2filenames\n",
        "\n",
        "# def read_filenames(root_dir):\n",
        "#     speaker2filenames = defaultdict(lambda : [])\n",
        "#     for path in sorted(glob.glob(os.path.join(root_dir, '*/*'))):\n",
        "#         filename = path.strip().split('/')[-1]\n",
        "#         speaker_id, utt_id = re.match(r'p(\\d+)_(\\d+)\\.wav', filename).groups()\n",
        "#         speaker2filenames[speaker_id].append(path)\n",
        "#     return speaker2filenames\n",
        "\n",
        "\n",
        "def wave_feature_extraction(wav_file, sr):\n",
        "    y, sr = librosa.load(wav_file, sr)\n",
        "    y, _ = librosa.effects.trim(y, top_db=20)\n",
        "    return y\n",
        "\n",
        "def spec_feature_extraction(wav_file):\n",
        "    mel, mag = get_spectrograms(wav_file)\n",
        "    return mel, mag\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # MRH:\n",
        "    \"\"\"\n",
        "    data_dir = sys.argv[1]\n",
        "    speaker_info_path = sys.argv[2]\n",
        "    output_dir = sys.argv[3]\n",
        "    test_speakers = int(sys.argv[4])\n",
        "    test_proportion = float(sys.argv[5])\n",
        "    sample_rate = int(sys.argv[6])\n",
        "    n_utts_attr = int(sys.argv[7])\n",
        "    \"\"\"\n",
        "    #\n",
        "    output_dir=\"/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/\"  # MRH: You may modify the address\n",
        "    data_dir = \"/content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental/wav48/\" # MRH: You may modify the address\n",
        "    speaker_info_path = \"/content/drive/MyDrive/Dataset/VCTK-Corpus-MRH-Experimental/speaker-info.txt\" # MRH: You may modify the address\n",
        "    test_speakers = 20\n",
        "    test_proportion = 0.1\n",
        "    n_utt_attr = 5000\n",
        "\n",
        "    #\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    #\n",
        "\n",
        "\n",
        "    speaker_ids = read_speaker_info(speaker_info_path)\n",
        "    random.shuffle(speaker_ids)\n",
        "\n",
        "    train_speaker_ids = speaker_ids[:-test_speakers]\n",
        "    test_speaker_ids = speaker_ids[-test_speakers:]\n",
        "\n",
        "    speaker2filenames = read_filenames(data_dir)\n",
        "\n",
        "    train_path_list, in_test_path_list, out_test_path_list = [], [], []\n",
        "\n",
        "    for speaker in train_speaker_ids:\n",
        "        # print('mrh speaker= ',speaker)\n",
        "        path_list = speaker2filenames[speaker]\n",
        "        # print('mrh path_list= ',path_list)\n",
        "        random.shuffle(path_list)\n",
        "        test_data_size = int(len(path_list) * test_proportion)\n",
        "        train_path_list += path_list[:-test_data_size]\n",
        "        in_test_path_list += path_list[-test_data_size:]\n",
        "\n",
        "    with open(os.path.join(output_dir, 'in_test_files.txt'), 'w') as f:\n",
        "        for path in in_test_path_list:\n",
        "            f.write(f'{path}\\n')\n",
        "\n",
        "    for speaker in test_speaker_ids:\n",
        "        path_list = speaker2filenames[speaker]\n",
        "        out_test_path_list += path_list\n",
        "\n",
        "    with open(os.path.join(output_dir, 'out_test_files.txt'), 'w') as f:\n",
        "        for path in out_test_path_list:\n",
        "            f.write(f'{path}\\n')\n",
        "\n",
        "    for dset, path_list in zip(['train', 'in_test', 'out_test'], \\\n",
        "            [train_path_list, in_test_path_list, out_test_path_list]):\n",
        "        print(f'processing {dset} set, {len(path_list)} files')\n",
        "        data = {}\n",
        "        output_path = os.path.join(output_dir, f'{dset}.pkl')\n",
        "        all_train_data = []\n",
        "        for i, path in enumerate(sorted(path_list)):\n",
        "            if i % 500 == 0 or i == len(path_list) - 1:\n",
        "                print(f'processing {i} files')\n",
        "            filename = path.strip().split('/')[-1]\n",
        "            mel, mag = spec_feature_extraction(path)\n",
        "            data[filename] = mel\n",
        "            if dset == 'train' and i < n_utts_attr:\n",
        "                all_train_data.append(mel)\n",
        "\n",
        "\n",
        "\n",
        "        #\n",
        "                # In your Colab Cell 1, where you define parameters:\n",
        "        print(f\"n_utt_attr: {n_utt_attr}\") # Make sure this is a sensible number, like 1000 or more\n",
        "\n",
        "        # In your make_datasets_vctk.py script, just before the problematic lines:\n",
        "\n",
        "        # After speaker_ids = read_speaker_info(speaker_info_path)\n",
        "        print(f\"Total speakers found: {len(speaker_ids)}\")\n",
        "        print(f\"First few speaker IDs: {speaker_ids[:5]}\")\n",
        "\n",
        "        # After train_speaker_ids = speaker_ids[:-test_speakers]\n",
        "        print(f\"Number of train speakers: {len(train_speaker_ids)}\")\n",
        "        print(f\"Number of test speakers: {len(test_speaker_ids)}\")\n",
        "\n",
        "        # After speaker2filenames = read_filenames(data_dir)\n",
        "        print(f\"Total files found by read_filenames: {sum(len(v) for v in speaker2filenames.values())}\")\n",
        "        # Print a sample of files for a speaker (replace 'pXXX' with a speaker ID you expect to find)\n",
        "        # if 'p225' in speaker2filenames:\n",
        "        #    print(f\"Sample files for p225: {speaker2filenames['p225'][:5]}\")\n",
        "\n",
        "        # Just before the loop that populates train_path_list:\n",
        "        print(f\"Initial train_path_list length (before populating): {len(train_path_list)}\")\n",
        "        print(f\"Initial in_test_path_list length (before populating): {len(in_test_path_list)}\")\n",
        "\n",
        "        # After the loops that populate train_path_list and in_test_path_list:\n",
        "        print(f\"Final train_path_list length: {len(train_path_list)}\")\n",
        "        print(f\"Final in_test_path_list length: {len(in_test_path_list)}\")\n",
        "        print(f\"Final out_test_path_list length: {len(out_test_path_list)}\")\n",
        "        #\n",
        "\n",
        "        # raise Exception('Mohammad Reza')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if dset == 'train':\n",
        "            all_train_data = np.concatenate(all_train_data)\n",
        "            mean = np.mean(all_train_data, axis=0)\n",
        "            std = np.std(all_train_data, axis=0)\n",
        "            attr = {'mean': mean, 'std': std}\n",
        "            with open(os.path.join(output_dir, 'attr.pkl'), 'wb') as f:\n",
        "                pickle.dump(attr, f)\n",
        "        for key, val in data.items():\n",
        "            val = (val - mean) / std\n",
        "            data[key] = val\n",
        "        with open(output_path, 'wb') as f:\n",
        "            pickle.dump(data, f)"
      ],
      "metadata": {
        "id": "HHiUgg5bCx1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce_dataset.py\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "pkl_path = '/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/train.pkl' # MRH: You may modify the address\n",
        "segment_size=128\n",
        "output_path = f'/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/train_{segment_size}.pkl' # MRH: You may modify the address\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # pkl_path = sys.argv[1]\n",
        "    # output_path = sys.argv[2]\n",
        "    # segment_size = int(sys.argv[3])\n",
        "\n",
        "    with open(pkl_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    reduced_data = {key:val for key, val in data.items() if val.shape[0] > segment_size}\n",
        "\n",
        "    with open(output_path, 'wb') as f:\n",
        "        pickle.dump(reduced_data, f)"
      ],
      "metadata": {
        "id": "9ReFBDN7C1Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MRH= This is for train.pkl to produce train_samples_{segment_size}.json\n",
        "import os\n",
        "\n",
        "# Configuration variables\n",
        "training_samples = 10000000\n",
        "segment_size = 128\n",
        "pickle_path = '/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/train.pkl'  # MRH: You may modify the address\n",
        "\n",
        "# Construct sample_path using f-string\n",
        "sample_path = f'/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/train_samples_{segment_size}.json'  # MRH: You may modify the address\n",
        "\n",
        "\n",
        "print(f\"Configuration set:\")\n",
        "print(f\"  Training Samples: {training_samples}\")\n",
        "print(f\"  Segment Size: {segment_size}\")\n",
        "print(f\"  Pickle Path: {pickle_path}\")\n",
        "print(f\"  Sample Path: {sample_path}\")\n",
        "\n",
        "#---------------------#\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# The variables (training_samples, segment_size, pickle_path, sample_path)\n",
        "# are assumed to be defined in the previous cell.\n",
        "\n",
        "# Load the data from the pickle file\n",
        "print(f\"Loading data from {pickle_path}...\")\n",
        "with open(pickle_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# (utt_id, timestep)\n",
        "samples = []\n",
        "\n",
        "# Filter utterances with length > segment_size\n",
        "utt_list = [key for key in data]\n",
        "utt_list = sorted(list(filter(lambda u : len(data[u]) > segment_size, utt_list)))\n",
        "print(f'{len(utt_list)} utterances meet the segment size criteria.')\n",
        "\n",
        "# Generate samples\n",
        "print(f\"Generating {training_samples} samples...\")\n",
        "sample_utt_index_list = random.choices(range(len(utt_list)), k=training_samples)\n",
        "\n",
        "for i, utt_ind in enumerate(sample_utt_index_list):\n",
        "    if i % 500000 == 0: # Increased frequency for print to avoid too many outputs for large n_samples\n",
        "        print(f'Sampled {i}/{training_samples} samples...')\n",
        "    utt_id = utt_list[utt_ind]\n",
        "    # Ensure t is within valid bounds\n",
        "    t_max = len(data[utt_id]) - segment_size\n",
        "    if t_max < 0: # Should not happen if filtered correctly, but good for robustness\n",
        "        continue\n",
        "    t = random.randint(0, t_max)\n",
        "    samples.append((utt_id, t))\n",
        "\n",
        "# Save the samples to a JSON file\n",
        "print(f\"Saving {len(samples)} samples to {sample_path}...\")\n",
        "with open(sample_path, 'w') as f:\n",
        "    json.dump(samples, f)\n",
        "print(\"Sampling complete and saved.\")"
      ],
      "metadata": {
        "id": "AuUy6ot2DM0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MRH= This is for in_test.pkl to produce in_test_samples_{segment_size}.json\n",
        "import os\n",
        "\n",
        "# Configuration variables\n",
        "testing_samples = 10000\n",
        "segment_size = 128\n",
        "pickle_path = '/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/in_test.pkl' # MRH: You may modify the address\n",
        "\n",
        "# Construct sample_path using f-string\n",
        "sample_path = f'/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/in_test_samples_{segment_size}.json' # MRH: You may modify the address\n",
        "\n",
        "\n",
        "print(f\"Configuration set:\")\n",
        "print(f\"  Training Samples: {training_samples}\")\n",
        "print(f\"  Segment Size: {segment_size}\")\n",
        "print(f\"  Pickle Path: {pickle_path}\")\n",
        "print(f\"  Sample Path: {sample_path}\")\n",
        "\n",
        "#---------------------#\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# The variables (testing_samples, segment_size, pickle_path, sample_path)\n",
        "# are assumed to be defined in the previous cell.\n",
        "\n",
        "# Load the data from the pickle file\n",
        "print(f\"Loading data from {pickle_path}...\")\n",
        "with open(pickle_path, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "print(\"Data loaded successfully.\")\n",
        "\n",
        "# (utt_id, timestep)\n",
        "samples = []\n",
        "\n",
        "# Filter utterances with length > segment_size\n",
        "utt_list = [key for key in data]\n",
        "utt_list = sorted(list(filter(lambda u : len(data[u]) > segment_size, utt_list)))\n",
        "print(f'{len(utt_list)} utterances meet the segment size criteria.')\n",
        "\n",
        "# Generate samples\n",
        "print(f\"Generating {testing_samples} samples...\")\n",
        "sample_utt_index_list = random.choices(range(len(utt_list)), k=testing_samples)\n",
        "\n",
        "for i, utt_ind in enumerate(sample_utt_index_list):\n",
        "    if i % 500000 == 0: # Increased frequency for print to avoid too many outputs for large n_samples\n",
        "        print(f'Sampled {i}/{testing_samples} samples...')\n",
        "    utt_id = utt_list[utt_ind]\n",
        "    # Ensure t is within valid bounds\n",
        "    t_max = len(data[utt_id]) - segment_size\n",
        "    if t_max < 0: # Should not happen if filtered correctly, but good for robustness\n",
        "        continue\n",
        "    t = random.randint(0, t_max)\n",
        "    samples.append((utt_id, t))\n",
        "\n",
        "# Save the samples to a JSON file\n",
        "print(f\"Saving {len(samples)} samples to {sample_path}...\")\n",
        "with open(sample_path, 'w') as f:\n",
        "    json.dump(samples, f)\n",
        "print(\"Sampling complete and saved.\")"
      ],
      "metadata": {
        "id": "4pvh8NIIDRzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**MRH: As mentioned the Above part are implemented once to extract features, then the following part should be runned every time to produce resultst**"
      ],
      "metadata": {
        "id": "W20Fq9AWDpZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adaptive_voice_conversion /utils.py\n",
        "import torch\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "import editdistance\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "def cc(net):\n",
        "    # print('100')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # print('200')\n",
        "    return net.to(device)\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, logdir='./log'):\n",
        "        self.writer = SummaryWriter(logdir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        self.writer.add_scalar(tag, value, step)\n",
        "\n",
        "    def scalars_summary(self, tag, dictionary, step):\n",
        "        self.writer.add_scalars(tag, dictionary, step)\n",
        "\n",
        "    def text_summary(self, tag, value, step):\n",
        "        self.writer.add_text(tag, value, step)\n",
        "\n",
        "    def audio_summary(self, tag, value, step, sr):\n",
        "        writer.add_audio(tag, value, step, sample_rate=sr)\n",
        "\n",
        "def infinite_iter(iterable):\n",
        "    it = iter(iterable)\n",
        "    while True:\n",
        "        try:\n",
        "            ret = next(it)\n",
        "            yield ret\n",
        "        except StopIteration:\n",
        "            it = iter(iterable)"
      ],
      "metadata": {
        "id": "JtK-Z2ZyEXcW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Model"
      ],
      "metadata": {
        "id": "F7LXCCjFEirz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adaptive_voice_conversion/model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as ag\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from functools import reduce\n",
        "from torch.nn.utils import spectral_norm\n",
        "# from utils import cc\n",
        "\n",
        "class DummyEncoder(object):\n",
        "    def __init__(self, encoder):\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def load(self, target_network):\n",
        "        self.encoder.load_state_dict(target_network.state_dict())\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "def pad_layer(inp, layer, pad_type='reflect'):\n",
        "    kernel_size = layer.kernel_size[0]\n",
        "    if kernel_size % 2 == 0:\n",
        "        pad = (kernel_size//2, kernel_size//2 - 1)\n",
        "    else:\n",
        "        pad = (kernel_size//2, kernel_size//2)\n",
        "    # padding\n",
        "    inp = F.pad(inp,\n",
        "            pad=pad,\n",
        "            mode=pad_type)\n",
        "    out = layer(inp)\n",
        "    return out\n",
        "\n",
        "def pad_layer_2d(inp, layer, pad_type='reflect'):\n",
        "    kernel_size = layer.kernel_size\n",
        "    if kernel_size[0] % 2 == 0:\n",
        "        pad_lr = [kernel_size[0]//2, kernel_size[0]//2 - 1]\n",
        "    else:\n",
        "        pad_lr = [kernel_size[0]//2, kernel_size[0]//2]\n",
        "    if kernel_size[1] % 2 == 0:\n",
        "        pad_ud = [kernel_size[1]//2, kernel_size[1]//2 - 1]\n",
        "    else:\n",
        "        pad_ud = [kernel_size[1]//2, kernel_size[1]//2]\n",
        "    pad = tuple(pad_lr + pad_ud)\n",
        "    # padding\n",
        "    inp = F.pad(inp,\n",
        "            pad=pad,\n",
        "            mode=pad_type)\n",
        "    out = layer(inp)\n",
        "    return out\n",
        "\n",
        "def pixel_shuffle_1d(inp, scale_factor=2):\n",
        "    batch_size, channels, in_width = inp.size()\n",
        "    channels //= scale_factor\n",
        "    out_width = in_width * scale_factor\n",
        "    inp_view = inp.contiguous().view(batch_size, channels, scale_factor, in_width)\n",
        "    shuffle_out = inp_view.permute(0, 1, 3, 2).contiguous()\n",
        "    shuffle_out = shuffle_out.view(batch_size, channels, out_width)\n",
        "    return shuffle_out\n",
        "\n",
        "def upsample(x, scale_factor=2):\n",
        "    x_up = F.interpolate(x, scale_factor=scale_factor, mode='nearest')\n",
        "    return x_up\n",
        "\n",
        "def flatten(x):\n",
        "    out = x.contiguous().view(x.size(0), x.size(1) * x.size(2))\n",
        "    return out\n",
        "\n",
        "def concat_cond(x, cond):\n",
        "    # x = [batch_size, x_channels, length]\n",
        "    # cond = [batch_size, c_channels]\n",
        "    cond = cond.unsqueeze(dim=2)\n",
        "    cond = cond.expand(*cond.size()[:-1], x.size(-1))\n",
        "    out = torch.cat([x, cond], dim=1)\n",
        "    return out\n",
        "\n",
        "def append_cond(x, cond):\n",
        "    # x = [batch_size, x_channels, length]\n",
        "    # cond = [batch_size, x_channels * 2]\n",
        "    p = cond.size(1) // 2\n",
        "    mean, std = cond[:, :p], cond[:, p:]\n",
        "    out = x * std.unsqueeze(dim=2) + mean.unsqueeze(dim=2)\n",
        "    return out\n",
        "\n",
        "def conv_bank(x, module_list, act, pad_type='reflect'):\n",
        "    outs = []\n",
        "    for layer in module_list:\n",
        "        out = act(pad_layer(x, layer, pad_type))\n",
        "        outs.append(out)\n",
        "    out = torch.cat(outs + [x], dim=1)\n",
        "    return out\n",
        "\n",
        "def get_act(act):\n",
        "    if act == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif act == 'lrelu':\n",
        "        return nn.LeakyReLU()\n",
        "    else:\n",
        "        return nn.ReLU()\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, c_in, c_h, n_blocks, act, sn):\n",
        "        super(MLP, self).__init__()\n",
        "        self.act = get_act(act)\n",
        "        self.n_blocks = n_blocks\n",
        "        f = spectral_norm if sn else lambda x: x\n",
        "        self.in_dense_layer = f(nn.Linear(c_in, c_h))\n",
        "        self.first_dense_layers = nn.ModuleList([f(nn.Linear(c_h, c_h)) for _ in range(n_blocks)])\n",
        "        self.second_dense_layers = nn.ModuleList([f(nn.Linear(c_h, c_h)) for _ in range(n_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.in_dense_layer(x)\n",
        "        for l in range(self.n_blocks):\n",
        "            y = self.first_dense_layers[l](h)\n",
        "            y = self.act(y)\n",
        "            y = self.second_dense_layers[l](y)\n",
        "            y = self.act(y)\n",
        "            h = h + y\n",
        "        return h\n",
        "\n",
        "class Prenet(nn.Module):\n",
        "    def __init__(self, c_in, c_h, c_out,\n",
        "            kernel_size, n_conv_blocks,\n",
        "            subsample, act, dropout_rate):\n",
        "        super(Prenet, self).__init__()\n",
        "        self.act = get_act(act)\n",
        "        self.subsample = subsample\n",
        "        self.n_conv_blocks = n_conv_blocks\n",
        "        self.in_conv_layer = nn.Conv2d(1, c_h, kernel_size=kernel_size)\n",
        "        self.first_conv_layers = nn.ModuleList([nn.Conv2d(c_h, c_h, kernel_size=kernel_size) for _ \\\n",
        "                in range(n_conv_blocks)])\n",
        "        self.second_conv_layers = nn.ModuleList([nn.Conv2d(c_h, c_h, kernel_size=kernel_size, stride=sub)\n",
        "            for sub, _ in zip(subsample, range(n_conv_blocks))])\n",
        "        output_size = c_in\n",
        "        for l, sub in zip(range(n_conv_blocks), self.subsample):\n",
        "            output_size = ceil(output_size / sub)\n",
        "        self.out_conv_layer = nn.Conv1d(c_h * output_size, c_out, kernel_size=1)\n",
        "        self.dropout_layer = nn.Dropout(p=dropout_rate)\n",
        "        self.norm_layer = nn.InstanceNorm2d(c_h, affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # reshape x to 4D\n",
        "        x = x.contiguous().view(x.size(0), 1, x.size(1), x.size(2))\n",
        "        out = pad_layer_2d(x, self.in_conv_layer)\n",
        "        out = self.act(out)\n",
        "        out = self.norm_layer(out)\n",
        "        for l in range(self.n_conv_blocks):\n",
        "            y = pad_layer_2d(out, self.first_conv_layers[l])\n",
        "            y = self.act(y)\n",
        "            y = self.norm_layer(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            y = pad_layer_2d(y, self.second_conv_layers[l])\n",
        "            y = self.act(y)\n",
        "            y = self.norm_layer(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            if self.subsample[l] > 1:\n",
        "                out = F.avg_pool2d(out, kernel_size=self.subsample[l], ceil_mode=True)\n",
        "            out = y + out\n",
        "        out = out.contiguous().view(out.size(0), out.size(1) * out.size(2), out.size(3))\n",
        "        out = pad_layer(out, self.out_conv_layer)\n",
        "        out = self.act(out)\n",
        "        return out\n",
        "\n",
        "class Postnet(nn.Module):\n",
        "    def __init__(self, c_in, c_h, c_out, c_cond,\n",
        "            kernel_size, n_conv_blocks,\n",
        "            upsample, act, sn):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.act = get_act(act)\n",
        "        self.upsample = upsample\n",
        "        self.c_h = c_h\n",
        "        self.n_conv_blocks = n_conv_blocks\n",
        "        f = spectral_norm if sn else lambda x: x\n",
        "        total_upsample = reduce(lambda x, y: x*y, upsample)\n",
        "        self.in_conv_layer = f(nn.Conv1d(c_in, c_h * c_out // total_upsample, kernel_size=1))\n",
        "        self.first_conv_layers = nn.ModuleList([f(nn.Conv2d(c_h, c_h, kernel_size=kernel_size)) for _ \\\n",
        "                in range(n_conv_blocks)])\n",
        "        self.second_conv_layers = nn.ModuleList([f(nn.Conv2d(c_h, c_h*up*up, kernel_size=kernel_size))\n",
        "            for up, _ in zip(upsample, range(n_conv_blocks))])\n",
        "        self.out_conv_layer = f(nn.Conv2d(c_h, 1, kernel_size=1))\n",
        "        self.conv_affine_layers = nn.ModuleList(\n",
        "                [f(nn.Linear(c_cond, c_h * 2)) for _ in range(n_conv_blocks*2)])\n",
        "        self.norm_layer = nn.InstanceNorm2d(c_h, affine=False)\n",
        "        self.ps = nn.PixelShuffle(max(upsample))\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        out = pad_layer(x, self.in_conv_layer)\n",
        "        out = out.contiguous().view(out.size(0), self.c_h, out.size(1) // self.c_h, out.size(2))\n",
        "        for l in range(self.n_conv_blocks):\n",
        "            y = pad_layer_2d(out, self.first_conv_layers[l])\n",
        "            y = self.act(y)\n",
        "            y = self.norm_layer(y)\n",
        "            y = append_cond_2d(y, self.conv_affine_layers[l*2](cond))\n",
        "            y = pad_layer_2d(y, self.second_conv_layers[l])\n",
        "            y = self.act(y)\n",
        "            if self.upsample[l] > 1:\n",
        "                y = self.ps(y)\n",
        "                y = self.norm_layer(y)\n",
        "                y = append_cond_2d(y, self.conv_affine_layers[l*2+1](cond))\n",
        "                out = y + upsample(out, scale_factor=(self.upsample[l], self.upsample[l]))\n",
        "            else:\n",
        "                y = self.norm_layer(y)\n",
        "                y = append_cond(y, self.conv_affine_layers[l*2+1](cond))\n",
        "                out = y + out\n",
        "        out = self.out_conv_layer(out)\n",
        "        out = out.squeeze(dim=1)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "D5AiWWc9EjMY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeakerEncoder1(nn.Module):\n",
        "    def __init__(self, c_in, c_h, c_out, kernel_size,\n",
        "            bank_size, bank_scale, c_bank,\n",
        "            n_conv_blocks, n_dense_blocks,\n",
        "            subsample, act, dropout_rate,\n",
        "            use_hierarchy=False, n_levels=1):\n",
        "        super(SpeakerEncoder1, self).__init__()\n",
        "        self.c_in = c_in\n",
        "        self.c_h = c_h\n",
        "        self.c_out = c_out # Dimension for each individual latent level (z1, z2, etc.)\n",
        "        self.kernel_size = kernel_size\n",
        "        self.n_conv_blocks = n_conv_blocks\n",
        "        self.n_dense_blocks = n_dense_blocks\n",
        "        self.subsample = subsample\n",
        "        self.act = get_act(act)\n",
        "        self.use_hierarchy = use_hierarchy\n",
        "        self.n_levels = n_levels\n",
        "\n",
        "        self.conv_bank = nn.ModuleList(\n",
        "                [nn.Conv1d(c_in, c_bank, kernel_size=k) for k in range(bank_scale, bank_size + 1, bank_scale)])\n",
        "        in_channels = c_bank * (bank_size // bank_scale) + c_in\n",
        "        self.in_conv_layer = nn.Conv1d(in_channels, c_h, kernel_size=1)\n",
        "\n",
        "        # Convolutional blocks\n",
        "        self.first_conv_layers = nn.ModuleList([nn.Conv1d(c_h, c_h, kernel_size=kernel_size) for _ in range(n_conv_blocks)])\n",
        "        self.second_conv_layers = nn.ModuleList([nn.Conv1d(c_h, c_h, kernel_size=kernel_size, stride=sub) for sub in subsample])\n",
        "\n",
        "        self.pooling_layer = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Dense blocks\n",
        "        self.first_dense_layers = nn.ModuleList([nn.Linear(c_h, c_h) for _ in range(n_dense_blocks)])\n",
        "        self.second_dense_layers = nn.ModuleList([nn.Linear(c_h, c_h) for _ in range(n_dense_blocks)])\n",
        "\n",
        "        if self.use_hierarchy:\n",
        "            # For each level (z1, z2, ..., zn), we need posterior mu and log_sigma layers\n",
        "            # All levels have c_out dimensions\n",
        "            self.mu_layers = nn.ModuleList([nn.Linear(c_h, c_out) for _ in range(n_levels)])\n",
        "            self.log_sigma_layers = nn.ModuleList([nn.Linear(c_h, c_out) for _ in range(n_levels)])\n",
        "\n",
        "            # Prior predictor layers: `prior_predictor_layers[k]` maps from z_{k+1} to prior params for z_k\n",
        "            # e.g., prior_predictor_layers[0] for z1_prior from z2.\n",
        "            #       prior_predictor_layers[1] for z2_prior from z3.\n",
        "            self.prior_predictor_layers = nn.ModuleList()\n",
        "            for _ in range(n_levels - 1): # (n_levels - 1) prior networks needed\n",
        "                self.prior_predictor_layers.append(nn.Sequential(\n",
        "                    nn.Linear(c_out, c_h),\n",
        "                    self.act,\n",
        "                    nn.Linear(c_h, c_out * 2) # For mu_prior and log_sigma_prior\n",
        "                ))\n",
        "        else:\n",
        "            self.output_layer = nn.Linear(c_h, c_out)\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def conv_blocks(self, inp):\n",
        "        out = inp\n",
        "        for l in range(self.n_conv_blocks):\n",
        "            y = pad_layer(out, self.first_conv_layers[l])\n",
        "            y = self.act(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            y = pad_layer(y, self.second_conv_layers[l])\n",
        "            y = self.act(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            if self.subsample[l] > 1:\n",
        "                out = F.avg_pool1d(out, kernel_size=self.subsample[l], ceil_mode=True)\n",
        "            out = y + out # Residual connection\n",
        "        return out\n",
        "\n",
        "    def dense_blocks(self, inp):\n",
        "        out = inp\n",
        "        for l in range(self.n_dense_blocks):\n",
        "            y = self.first_dense_layers[l](out)\n",
        "            y = self.act(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            y = self.second_dense_layers[l](y)\n",
        "            y = self.act(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            out = y + out # Residual connection\n",
        "        return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = conv_bank(x, self.conv_bank, act=self.act)\n",
        "        out = pad_layer(out, self.in_conv_layer)\n",
        "        out = self.act(out)\n",
        "        out = self.conv_blocks(out)\n",
        "        out = self.pooling_layer(out).squeeze(2) # Shape: (batch_size, c_h)\n",
        "        pooled_features = self.dense_blocks(out) # pooled_features is (batch_size, c_h)\n",
        "\n",
        "        if self.use_hierarchy:\n",
        "            mus = []\n",
        "            log_sigmas = []\n",
        "            mu_priors = [] # [None (for z1), prior for z2 (from z1), prior for z3 (from z2)]\n",
        "            log_sigma_priors = [] # Same\n",
        "            sampled_latents = []\n",
        "\n",
        "            # --- For z1 (Highest Level - standard normal prior) ---\n",
        "            # Posterior for z1 from pooled_features\n",
        "            mu_post_z1 = self.mu_layers[0](pooled_features)\n",
        "            log_sigma_post_z1 = self.log_sigma_layers[0](pooled_features)\n",
        "\n",
        "            # Sample z1\n",
        "            eps_z1 = torch.randn_like(mu_post_z1)\n",
        "            z1 = mu_post_z1 + torch.exp(log_sigma_post_z1 * 0.5) * eps_z1\n",
        "\n",
        "            mus.append(mu_post_z1)\n",
        "            log_sigmas.append(log_sigma_post_z1)\n",
        "            sampled_latents.append(z1)\n",
        "\n",
        "            # Prior for z1 is standard normal (N(0,I))\n",
        "            mu_priors.append(None)\n",
        "            log_sigma_priors.append(None)\n",
        "\n",
        "            # --- For subsequent levels (z2, z3, etc.) ---\n",
        "            # Loop from the second level (index 1) up to n_levels-1\n",
        "            for i in range(1, self.n_levels):\n",
        "                # Posterior for z_{i+1} from pooled_features\n",
        "                mu_post_zi = self.mu_layers[i](pooled_features)\n",
        "                log_sigma_post_zi = self.log_sigma_layers[i](pooled_features)\n",
        "\n",
        "                # Sample z_{i+1}\n",
        "                eps_zi = torch.randn_like(mu_post_zi)\n",
        "                zi = mu_post_zi + torch.exp(log_sigma_post_zi * 0.5) * eps_zi\n",
        "\n",
        "                mus.append(mu_post_zi)\n",
        "                log_sigmas.append(log_sigma_post_zi)\n",
        "                sampled_latents.append(zi) # sampled_latents[i] is z_{i+1}\n",
        "\n",
        "                # Compute prior for z_{i+1} from z_i (sampled_latents[i-1])\n",
        "                prior_params = self.prior_predictor_layers[i-1](sampled_latents[i-1])\n",
        "                mu_prior_zi, log_sigma_prior_zi = prior_params.chunk(2, dim=-1)\n",
        "\n",
        "                mu_priors.append(mu_prior_zi)\n",
        "                log_sigma_priors.append(log_sigma_prior_zi)\n",
        "\n",
        "            # The decoder input for speaker is the concatenation of all sampled speaker latents\n",
        "            decoder_input_emb = torch.cat(sampled_latents, dim=-1)\n",
        "\n",
        "            return {\n",
        "                'mus': mus, # List of posterior means [mu_z1, mu_z2, mu_z3]\n",
        "                'log_sigmas': log_sigmas, # List of posterior log_sigmas [log_sigma_z1, log_sigma_z2, log_sigma_z3]\n",
        "                'mu_priors': mu_priors, # List of priors [None (for z1), prior_z2_from_z1, prior_z3_from_z2]\n",
        "                'log_sigma_priors': log_sigma_priors, # Same\n",
        "                'decoder_input': decoder_input_emb # Concatenated speaker embedding (z1 || z2 || z3 ...)\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            # Original non-hierarchical output remains the same\n",
        "            emb = self.output_layer(pooled_features)\n",
        "            return {\n",
        "                'mus': [emb],\n",
        "                'log_sigmas': [torch.zeros_like(emb)],\n",
        "                'mu_priors': [None],\n",
        "                'log_sigma_priors': [None],\n",
        "                'decoder_input': emb\n",
        "            }"
      ],
      "metadata": {
        "id": "7JxlQAr3Ep-u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ContentEncoder1(nn.Module):\n",
        "    def __init__(self, c_in, c_h, c_out, kernel_size,\n",
        "            bank_size, bank_scale, c_bank,\n",
        "            n_conv_blocks, subsample,\n",
        "            act, dropout_rate,\n",
        "            use_hierarchy=False, n_levels=1):\n",
        "        super(ContentEncoder1, self).__init__()\n",
        "        self.n_conv_blocks = n_conv_blocks\n",
        "        self.subsample = subsample\n",
        "        self.act = get_act(act)\n",
        "        self.use_hierarchy = use_hierarchy\n",
        "        self.n_levels = n_levels\n",
        "        self.c_out = c_out # Dimension for each individual latent level (z1, z2, etc.)\n",
        "\n",
        "        self.conv_bank = nn.ModuleList(\n",
        "                [nn.Conv1d(c_in, c_bank, kernel_size=k) for k in range(bank_scale, bank_size + 1, bank_scale)])\n",
        "        in_channels = c_bank * (bank_size // bank_scale) + c_in\n",
        "        self.in_conv_layer = nn.Conv1d(in_channels, c_h, kernel_size=1)\n",
        "        self.first_conv_layers = nn.ModuleList([nn.Conv1d(c_h, c_h, kernel_size=kernel_size) for _ in range(n_conv_blocks)])\n",
        "        self.second_conv_layers = nn.ModuleList([nn.Conv1d(c_h, c_h, kernel_size=kernel_size, stride=sub) for sub in subsample])\n",
        "        self.norm_layer = nn.InstanceNorm1d(c_h, affine=False)\n",
        "        self.dropout_layer = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        if self.use_hierarchy:\n",
        "            # For each level (z1, z2, ..., zn), we need posterior mu and log_sigma layers\n",
        "            # All levels have c_out dimensions\n",
        "            self.mu_layers = nn.ModuleList([nn.Conv1d(c_h, c_out, kernel_size=1) for _ in range(n_levels)])\n",
        "            self.log_sigma_layers = nn.ModuleList([nn.Conv1d(c_h, c_out, kernel_size=1) for _ in range(n_levels)])\n",
        "\n",
        "            # Prior predictor layers: `prior_predictor_layers[k]` maps from z_{k+1} to prior params for z_k\n",
        "            self.prior_predictor_layers = nn.ModuleList()\n",
        "            for _ in range(n_levels - 1):\n",
        "                self.prior_predictor_layers.append(nn.Sequential(\n",
        "                    nn.Conv1d(c_out, c_h, kernel_size=1),\n",
        "                    self.act,\n",
        "                    nn.Conv1d(c_h, c_out * 2, kernel_size=1)\n",
        "                ))\n",
        "        else:\n",
        "            self.mean_layer = nn.Conv1d(c_h, c_out, kernel_size=1)\n",
        "            self.std_layer = nn.Conv1d(c_h, c_out, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = conv_bank(x, self.conv_bank, act=self.act)\n",
        "        out = pad_layer(out, self.in_conv_layer) # This is correct for in_conv_layer\n",
        "        out = self.norm_layer(out)\n",
        "        out = self.act(out)\n",
        "        out = self.dropout_layer(out)\n",
        "        # convolution blocks\n",
        "        for l in range(self.n_conv_blocks):\n",
        "            y = pad_layer(out, self.first_conv_layers[l]) # Correct\n",
        "            y = self.norm_layer(y)\n",
        "            y = self.act(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            y = pad_layer(y, self.second_conv_layers[l]) # Correct\n",
        "            y = self.norm_layer(y)\n",
        "            y = self.act(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            if self.subsample[l] > 1:\n",
        "                out = F.avg_pool1d(out, kernel_size=self.subsample[l], ceil_mode=True)\n",
        "            out = y + out # Residual connection\n",
        "\n",
        "        processed_features = out # (batch_size, c_h, seq_len)\n",
        "\n",
        "        if self.use_hierarchy:\n",
        "            mus = []\n",
        "            log_sigmas = []\n",
        "            mu_priors = [] # [None (for z1), prior for z2 (from z1), prior for z3 (from z2)]\n",
        "            log_sigma_priors = [] # Same\n",
        "            sampled_latents = []\n",
        "\n",
        "            # --- For z1 (Highest Level - standard normal prior) ---\n",
        "            # Posterior for z1 from processed_features\n",
        "            # These are 1x1 convs, no need for external pad_layer.\n",
        "            mu_post_z1 = self.mu_layers[0](processed_features)\n",
        "            log_sigma_post_z1 = self.log_sigma_layers[0](processed_features)\n",
        "\n",
        "            # Sample z1\n",
        "            eps_z1 = torch.randn_like(mu_post_z1)\n",
        "            z1 = mu_post_z1 + torch.exp(log_sigma_post_z1 * 0.5) * eps_z1\n",
        "\n",
        "            mus.append(mu_post_z1)\n",
        "            log_sigmas.append(log_sigma_post_z1)\n",
        "            sampled_latents.append(z1)\n",
        "\n",
        "            # Prior for z1 is standard normal (N(0,I))\n",
        "            mu_priors.append(None)\n",
        "            log_sigma_priors.append(None)\n",
        "\n",
        "            # --- For subsequent levels (z2, z3, etc.) ---\n",
        "            # Loop from the second level (index 1) up to n_levels-1\n",
        "            for i in range(1, self.n_levels):\n",
        "                # Posterior for z_{i+1} from processed_features\n",
        "                # These are 1x1 convs, no need for external pad_layer.\n",
        "                mu_post_zi = self.mu_layers[i](processed_features)\n",
        "                log_sigma_post_zi = self.log_sigma_layers[i](processed_features)\n",
        "\n",
        "                # Sample z_{i+1}\n",
        "                eps_zi = torch.randn_like(mu_post_zi)\n",
        "                zi = mu_post_zi + torch.exp(log_sigma_post_zi * 0.5) * eps_zi\n",
        "\n",
        "                mus.append(mu_post_zi)\n",
        "                log_sigmas.append(log_sigma_post_zi)\n",
        "                sampled_latents.append(zi) # sampled_latents[i] is z_{i+1}\n",
        "\n",
        "                # Compute prior for z_{i+1} from z_i (sampled_latents[i-1])\n",
        "                # prior_predictor_layers[0] for z2's prior from z1\n",
        "                # prior_predictor_layers[1] for z3's prior from z2\n",
        "                # So, prior_predictor_layers[i-1] takes sampled_latents[i-1] (z_i)\n",
        "                # FIX: Remove pad_layer here. Just call the Sequential module directly.\n",
        "                prior_params = self.prior_predictor_layers[i-1](sampled_latents[i-1])\n",
        "                mu_prior_zi, log_sigma_prior_zi = prior_params.chunk(2, dim=1)\n",
        "\n",
        "                mu_priors.append(mu_prior_zi)\n",
        "                log_sigma_priors.append(log_sigma_prior_zi)\n",
        "\n",
        "            # The decoder input is always z1 (the highest level latent)\n",
        "            decoder_input_content = sampled_latents[0]\n",
        "\n",
        "            return {\n",
        "                'mus': mus, # List of posterior means [mu_z1, mu_z2, mu_z3]\n",
        "                'log_sigmas': log_sigmas, # List of posterior log_sigmas [log_sigma_z1, log_sigma_z2, log_sigma_z3]\n",
        "                'mu_priors': mu_priors, # List of priors [None (for z1), prior_z2_from_z1, prior_z3_from_z2]\n",
        "                'log_sigma_priors': log_sigma_priors, # Same\n",
        "                'decoder_input': decoder_input_content # z1 only\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            # Original non-hierarchical output remains the same\n",
        "            # These are 1x1 convs, no need for external pad_layer.\n",
        "            mu = self.mean_layer(processed_features)\n",
        "            log_sigma = self.std_layer(processed_features)\n",
        "\n",
        "            eps = torch.randn_like(mu)\n",
        "            z = mu + torch.exp(log_sigma * 0.5) * eps\n",
        "\n",
        "            return {\n",
        "                'mus': [mu],\n",
        "                'log_sigmas': [log_sigma],\n",
        "                'mu_priors': [None],\n",
        "                'log_sigma_priors': [None],\n",
        "                'decoder_input': z\n",
        "            }"
      ],
      "metadata": {
        "id": "IESTCBnVEt28"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "            c_in, c_cond, c_h, c_out,\n",
        "            kernel_size,\n",
        "            n_conv_blocks, upsample, act, sn, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_conv_blocks = n_conv_blocks\n",
        "        self.upsample = upsample\n",
        "        self.act = get_act(act)\n",
        "        f = spectral_norm if sn else lambda x: x\n",
        "        self.in_conv_layer = f(nn.Conv1d(c_in, c_h, kernel_size=1))\n",
        "        self.first_conv_layers = nn.ModuleList([f(nn.Conv1d(c_h, c_h, kernel_size=kernel_size)) for _ \\\n",
        "                in range(n_conv_blocks)])\n",
        "        self.second_conv_layers = nn.ModuleList(\\\n",
        "                [f(nn.Conv1d(c_h, c_h * up, kernel_size=kernel_size)) \\\n",
        "                for _, up in zip(range(n_conv_blocks), self.upsample)])\n",
        "        self.norm_layer = nn.InstanceNorm1d(c_h, affine=False)\n",
        "        self.conv_affine_layers = nn.ModuleList(\n",
        "                [f(nn.Linear(c_cond, c_h * 2)) for _ in range(n_conv_blocks*2)])\n",
        "        self.out_conv_layer = f(nn.Conv1d(c_h, c_out, kernel_size=1))\n",
        "        self.dropout_layer = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, z, cond):\n",
        "        out = pad_layer(z, self.in_conv_layer)\n",
        "        out = self.norm_layer(out)\n",
        "        out = self.act(out)\n",
        "        out = self.dropout_layer(out)\n",
        "        # convolution blocks\n",
        "        for l in range(self.n_conv_blocks):\n",
        "            y = pad_layer(out, self.first_conv_layers[l])\n",
        "            y = self.norm_layer(y)\n",
        "            y = append_cond(y, self.conv_affine_layers[l*2](cond))\n",
        "            y = self.act(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            y = pad_layer(y, self.second_conv_layers[l])\n",
        "            if self.upsample[l] > 1:\n",
        "                y = pixel_shuffle_1d(y, scale_factor=self.upsample[l])\n",
        "            y = self.norm_layer(y)\n",
        "            y = append_cond(y, self.conv_affine_layers[l*2+1](cond))\n",
        "            y = self.act(y)\n",
        "            y = self.dropout_layer(y)\n",
        "            if self.upsample[l] > 1:\n",
        "                out = y + upsample(out, scale_factor=self.upsample[l])\n",
        "            else:\n",
        "                out = y + out\n",
        "        out = pad_layer(out, self.out_conv_layer)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "olpnBvHhE0lS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AE1(nn.Module):\n",
        "    def __init__(self,  args):\n",
        "        super(AE1, self).__init__()\n",
        "        self.use_content_hierarchy = args.use_content_hierarchy\n",
        "        self.n_content_levels = args.n_content_levels if args.use_content_hierarchy else 1\n",
        "        self.use_speaker_hierarchy = args.use_speaker_hierarchy\n",
        "        self.n_speaker_levels = args.n_speaker_levels if args.use_speaker_hierarchy else 1\n",
        "\n",
        "        # Configure SpeakerEncoder\n",
        "        speaker_encoder_config = args.config['SpeakerEncoder'].copy()\n",
        "        speaker_encoder_config['use_hierarchy'] = self.use_speaker_hierarchy\n",
        "        speaker_encoder_config['n_levels'] = self.n_speaker_levels\n",
        "        self.speaker_encoder = SpeakerEncoder1(**speaker_encoder_config)\n",
        "\n",
        "        # Configure ContentEncoder\n",
        "        content_encoder_config = args.config['ContentEncoder'].copy()\n",
        "        content_encoder_config['use_hierarchy'] = self.use_content_hierarchy\n",
        "        content_encoder_config['n_levels'] = self.n_content_levels\n",
        "        self.content_encoder = ContentEncoder1(**content_encoder_config)\n",
        "\n",
        "        # Configure Decoder\n",
        "        decoder_config = args.config['Decoder'].copy()\n",
        "\n",
        "        # Dynamically set c_in for decoder (input from ContentEncoder)\n",
        "        decoder_config['c_in'] = content_encoder_config['c_out'] # z1 of content is always c_out\n",
        "\n",
        "        # Dynamically set c_cond for decoder (input from SpeakerEncoder)\n",
        "        if self.use_speaker_hierarchy:\n",
        "            # Speaker decoder input is the concatenation of all speaker latents (z1 || z2 || z3 ...)\n",
        "            decoder_config['c_cond'] = speaker_encoder_config['c_out'] * self.n_speaker_levels\n",
        "        else:\n",
        "            decoder_config['c_cond'] = speaker_encoder_config['c_out']\n",
        "\n",
        "        self.decoder = Decoder(**decoder_config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        speaker_latents_info = self.speaker_encoder(x)\n",
        "        content_latents_info = self.content_encoder(x)\n",
        "\n",
        "        # Decoder receives the specific 'decoder_input' from the encoder.\n",
        "        # For speaker, it's the concatenated embedding.\n",
        "        # For content, it's z1.\n",
        "        speaker_decoder_input = speaker_latents_info['decoder_input']\n",
        "        content_decoder_input = content_latents_info['decoder_input']\n",
        "\n",
        "        dec = self.decoder(content_decoder_input, speaker_decoder_input)\n",
        "\n",
        "        return {\n",
        "            'dec': dec,\n",
        "            'speaker_latents': speaker_latents_info,\n",
        "            'content_latents': content_latents_info,\n",
        "        }\n",
        "\n",
        "    def inference(self, x, x_cond):\n",
        "        speaker_latents_info = self.speaker_encoder(x_cond)\n",
        "        content_latents_info = self.content_encoder(x)\n",
        "\n",
        "        # For inference, use the posterior mean (mu) of z1 for content, as z1 is reconstructed.\n",
        "        content_decoder_input = content_latents_info['mus'][0]\n",
        "\n",
        "        # For speaker, use the `decoder_input` which is the concatenated embedding.\n",
        "        speaker_decoder_input = speaker_latents_info['decoder_input']\n",
        "\n",
        "        dec = self.decoder(content_decoder_input, speaker_decoder_input)\n",
        "        return dec\n",
        "\n",
        "    def get_speaker_embeddings(self, x):\n",
        "        # This function should return the final speaker embedding used by the decoder.\n",
        "        speaker_latents_info = self.speaker_encoder(x)\n",
        "        return speaker_latents_info['decoder_input']"
      ],
      "metadata": {
        "id": "BvMacmDEE6vL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "###### Code    ########\n",
        "#######################"
      ],
      "metadata": {
        "id": "6n1z99SvE8FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adaptive_voice_conversion/data_utils.py\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CollateFn(object):\n",
        "    def __init__(self, frame_size):\n",
        "        self.frame_size = frame_size\n",
        "\n",
        "    def make_frames(self, tensor):\n",
        "        out = tensor.view(tensor.size(0), tensor.size(1) // self.frame_size, self.frame_size * tensor.size(2))\n",
        "        out = out.transpose(1, 2)\n",
        "        return out\n",
        "\n",
        "    def __call__(self, l):\n",
        "        data_tensor = torch.from_numpy(np.array(l))\n",
        "        segment = self.make_frames(data_tensor)\n",
        "        return segment\n",
        "\n",
        "def get_data_loader(dataset, batch_size, frame_size, shuffle=True, num_workers=4, drop_last=False):\n",
        "    _collate_fn = CollateFn(frame_size=frame_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "            num_workers=num_workers, collate_fn=_collate_fn, pin_memory=True)\n",
        "    return dataloader\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.utt_ids = list(self.data.keys())\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        utt_id = self.utt_ids[ind]\n",
        "        ret = self.data[utt_id].transpose()\n",
        "        return ret\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.utt_ids)\n",
        "\n",
        "class PickleDataset(Dataset):\n",
        "    def __init__(self, pickle_path, sample_index_path, segment_size):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        with open(sample_index_path, 'r') as f:\n",
        "            self.indexes = json.load(f)\n",
        "        self.segment_size = segment_size\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        utt_id, t = self.indexes[ind]\n",
        "        segment = self.data[utt_id][t:t + self.segment_size]\n",
        "        return segment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexes)\n"
      ],
      "metadata": {
        "id": "clQRcjhBFAqu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AELoss:\n",
        "    def __init__(self, n_speaker_levels: int, n_content_levels: int):\n",
        "        \"\"\"\n",
        "        Initializes the AELoss class.\n",
        "\n",
        "        Args:\n",
        "            n_speaker_levels: Number of speaker hierarchy levels.\n",
        "            n_content_levels: Number of content hierarchy levels.\n",
        "        \"\"\"\n",
        "        self.n_speaker_levels = n_speaker_levels\n",
        "        self.n_content_levels = n_content_levels\n",
        "\n",
        "    def calculate_kld(self, mu_post, log_sigma_post, mu_prior=None, log_sigma_prior=None):\n",
        "        \"\"\"\n",
        "        Calculates the Kullback-Leibler Divergence (KLD).\n",
        "        \"\"\"\n",
        "        if mu_prior is None or log_sigma_prior is None:\n",
        "            # Standard normal prior (N(0, I))\n",
        "            kld = -0.5 * torch.sum(1 + log_sigma_post - mu_post.pow(2) - log_sigma_post.exp())\n",
        "        else:\n",
        "            # Gaussian prior (N(mu_prior, exp(log_sigma_prior)))\n",
        "            kld = 0.5 * torch.sum(\n",
        "                torch.exp(log_sigma_post - log_sigma_prior) +\n",
        "                ((mu_prior - mu_post)**2) * torch.exp(-log_sigma_prior) - 1 +\n",
        "                (log_sigma_prior - log_sigma_post)\n",
        "            )\n",
        "        return kld\n",
        "\n",
        "    def speaker_kld_loss(self, speaker_latents):\n",
        "        \"\"\"\n",
        "        Calculates the KLD loss for the speaker hierarchy.\n",
        "        \"\"\"\n",
        "        speaker_kld_loss = 0\n",
        "        for i in range(self.n_speaker_levels):\n",
        "            mu_post = speaker_latents['mus'][i]\n",
        "            log_sigma_post = speaker_latents['log_sigmas'][i]\n",
        "            mu_prior = speaker_latents['mu_priors'][i]\n",
        "            log_sigma_prior = speaker_latents['log_sigma_priors'][i]\n",
        "            speaker_kld_loss += self.calculate_kld(mu_post, log_sigma_post, mu_prior, log_sigma_prior)\n",
        "        return speaker_kld_loss\n",
        "\n",
        "    def content_kld_loss(self, content_latents):\n",
        "        \"\"\"\n",
        "        Calculates the KLD loss for the content hierarchy.\n",
        "        \"\"\"\n",
        "        content_kld_loss = 0\n",
        "        for i in range(self.n_content_levels):\n",
        "            mu_post = content_latents['mus'][i]\n",
        "            log_sigma_post = content_latents['log_sigmas'][i]\n",
        "            mu_prior = content_latents['mu_priors'][i]\n",
        "            log_sigma_prior = content_latents['log_sigma_priors'][i]\n",
        "            content_kld_loss += self.calculate_kld(mu_post, log_sigma_post, mu_prior, log_sigma_prior)\n",
        "        return content_kld_loss\n",
        "\n",
        "    def loss_calculate(self, x, dec, speaker_latents, content_latents, lambda_kl: float = 1.0):\n",
        "        \"\"\"\n",
        "        Calculates the total loss for the Autoencoder.\n",
        "\n",
        "        Args:\n",
        "            x: Original input tensor.\n",
        "            dec: Reconstructed output tensor from the decoder.\n",
        "            speaker_latents: Dictionary containing speaker latent distributions.\n",
        "            content_latents: Dictionary containing content latent distributions.\n",
        "            lambda_kl: Weight for the KLD loss components.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing individual and total loss components.\n",
        "        \"\"\"\n",
        "        # Reconstruction Loss (e.g., L1)\n",
        "        recon_loss = F.l1_loss(dec, x)\n",
        "\n",
        "        speaker_kld_loss = self.speaker_kld_loss(speaker_latents)\n",
        "        content_kld_loss = self.content_kld_loss(content_latents)\n",
        "\n",
        "        # Total Loss with KLD weighting\n",
        "        total_loss = recon_loss + lambda_kl * (speaker_kld_loss + content_kld_loss)\n",
        "\n",
        "        return {\n",
        "            'total_loss': total_loss,\n",
        "            'recon_loss': recon_loss,\n",
        "            'speaker_kld_loss': speaker_kld_loss,\n",
        "            'content_kld_loss': content_kld_loss\n",
        "        }"
      ],
      "metadata": {
        "id": "P2xYb25aFGb2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "class ARGS1():\n",
        "  def __init__(self):\n",
        "    # Store the YAML content directly as a string\n",
        "    self.config_content = \"\"\"\n",
        "SpeakerEncoder:\n",
        "    c_in: 80\n",
        "    c_h: 128\n",
        "    c_out: 128\n",
        "    kernel_size: 5\n",
        "    bank_size: 8\n",
        "    bank_scale: 1\n",
        "    c_bank: 128\n",
        "    n_conv_blocks: 6\n",
        "    n_dense_blocks: 6\n",
        "    subsample: [1, 2, 1, 2, 1, 2]\n",
        "    act: 'relu'\n",
        "    dropout_rate: 0\n",
        "ContentEncoder:\n",
        "    c_in: 80\n",
        "    c_h: 128\n",
        "    c_out: 128\n",
        "    kernel_size: 5\n",
        "    bank_size: 8\n",
        "    bank_scale: 1\n",
        "    c_bank: 128\n",
        "    n_conv_blocks: 6\n",
        "    subsample: [1, 2, 1, 2, 1, 2]\n",
        "    act: 'relu'\n",
        "    dropout_rate: 0\n",
        "Decoder:\n",
        "    c_in: 128\n",
        "    c_cond: 128\n",
        "    c_h: 128\n",
        "    c_out: 80\n",
        "    kernel_size: 5\n",
        "    n_conv_blocks: 6\n",
        "    upsample: [2, 1, 2, 1, 2, 1]\n",
        "    act: 'relu'\n",
        "    sn: False\n",
        "    dropout_rate: 0\n",
        "data_loader:\n",
        "    segment_size: 128\n",
        "    frame_size: 1\n",
        "    batch_size: 128\n",
        "    shuffle: True\n",
        "optimizer:\n",
        "    lr: 0.0005\n",
        "    beta1: 0.9\n",
        "    beta2: 0.999\n",
        "    amsgrad: True\n",
        "    weight_decay: 0.0001\n",
        "    grad_norm: 5\n",
        "lambda:\n",
        "    lambda_rec: 10\n",
        "    lambda_kl: 1\n",
        "annealing_iters: 20000\n",
        "    \"\"\"\n",
        "    self.data_dir='/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/' # MRH: You may Modify the part\n",
        "    self.train_set='train_128'\n",
        "    self.train_index_file='train_samples_128.json'\n",
        "    self.logdir='log/'\n",
        "    self.load_model=False\n",
        "    self.store_model_path='/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/checkpoint' # MRH: You may Modify the part\n",
        "    self.load_model_path='/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/checkpoint' # MRH: You may Modify the part\n",
        "    self.summary_steps=500\n",
        "    self.save_steps=1000\n",
        "    self.tag='vctk_model'\n",
        "    self.iters=100000\n",
        "\n",
        "    #  Hierarchy\n",
        "\n",
        "    self.use_content_hierarchy=True\n",
        "    self.n_content_levels=3 # MRH: in the AE1,  if use_content_hierarchy= True then n_content_levels gets number else it is 1\n",
        "    self.use_speaker_hierarchy=True\n",
        "    self.n_speaker_levels=3 # MRH: in the AE1,  if use_speaker_hierarchy= True then n_content_levels gets number else it is 1\n",
        "\n",
        "    # Parse the YAML content into a dictionary\n",
        "    self.config = yaml.safe_load(self.config_content)\n",
        "\n",
        "##\n",
        "args1=ARGS1()\n",
        "print(args1.train_index_file)\n",
        "print(args1.data_dir)\n",
        "\n",
        "# Now you can access your config parameters like this:\n",
        "print(\"\\nAccessing config parameters:\")\n",
        "print(args1.config['SpeakerEncoder']['c_in'])\n",
        "print(args1.config['data_loader']['batch_size'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EC3mkgOFMSa",
        "outputId": "104db086-dbec-4d46-b568-2c62293cc974"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_samples_128.json\n",
            "/content/drive/MyDrive/AdaIN/mrh_AdaIN_Output_80bin/\n",
            "\n",
            "Accessing config parameters:\n",
            "80\n",
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzJv1QjeEbfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Performing Speaker Verification**\n",
        "\n",
        "**MRH: to include speaker id in output of reading data**"
      ],
      "metadata": {
        "id": "TJ-RSN8N2iaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Modify PickleDataset's __getitem__ to return both utt_id and segment:\n",
        "\n",
        "class PickleDataset(Dataset):\n",
        "    def __init__(self, pickle_path, sample_index_path, segment_size):\n",
        "        with open(pickle_path, 'rb') as f:\n",
        "            self.data = pickle.load(f)\n",
        "        with open(sample_index_path, 'r') as f:\n",
        "            self.indexes = json.load(f)\n",
        "        self.segment_size = segment_size\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        utt_id, t = self.indexes[ind]\n",
        "        segment = self.data[utt_id][t:t + self.segment_size]\n",
        "        return segment, utt_id  ###### MRH: This is the change: Return both segment and utt_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexes)"
      ],
      "metadata": {
        "id": "9G51-wK60rBk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Modify CollateFn's __call__ to handle both utt_id and segment\n",
        "class CollateFn(object):\n",
        "    def __init__(self, frame_size):\n",
        "        self.frame_size = frame_size\n",
        "\n",
        "    def make_frames(self, tensor):\n",
        "        out = tensor.view(tensor.size(0), tensor.size(1) // self.frame_size, self.frame_size * tensor.size(2))\n",
        "        out = out.transpose(1, 2)\n",
        "        return out\n",
        "\n",
        "    def __call__(self, l):\n",
        "        # l will be a list of (segment, utt_id) tuples from PickleDataset's __getitem__\n",
        "        # Correctly extract segments (numerical data) and utt_ids (strings)\n",
        "        numerical_segments = [item[0] for item in l] # This should be the numerical data\n",
        "        utt_ids = [item[1] for item in l]           # This should be the string utt_id\n",
        "\n",
        "        # Now, numerical_segments should be a list of NumPy arrays or similar,\n",
        "        # which can be converted to a single NumPy array and then to a torch.Tensor.\n",
        "        data_tensor = torch.from_numpy(np.array(numerical_segments))\n",
        "        segment = self.make_frames(data_tensor)\n",
        "\n",
        "        # Return the processed numerical segment tensor and the list of utt_ids\n",
        "        return segment, utt_ids # MRH: Return both segment and utt_id"
      ],
      "metadata": {
        "id": "DIy0dRXm0tvN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Speaker Verification Class**"
      ],
      "metadata": {
        "id": "VMK8Ufh53CGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tqdm.auto import tqdm\n",
        "import itertools # For generating pairs\n",
        "\n",
        "class SpeakerVerificationEvaluator:\n",
        "    def __init__(self, device='cpu'):\n",
        "        \"\"\"\n",
        "        Initializes the SpeakerVerificationEvaluator.\n",
        "\n",
        "        Args:\n",
        "            device (str): The device to perform calculations on ('cuda' or 'cpu').\n",
        "        \"\"\"\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "    def _extract_speaker_id_from_filename(self, utt_id_filename):\n",
        "        \"\"\"\n",
        "        Extracts the speaker ID from a filename like 'p229_018.wav'.\n",
        "        Assumes the speaker ID is the first part before the first underscore.\n",
        "        \"\"\"\n",
        "        if not isinstance(utt_id_filename, str):\n",
        "            raise TypeError(f\"Expected utt_id_filename to be a string, but got {type(utt_id_filename)}\")\n",
        "        return utt_id_filename.split('_')[0]\n",
        "\n",
        "    def _calculate_cosine_similarities(self, embeddings, speaker_ids_or_filenames):\n",
        "        \"\"\"\n",
        "        Calculates cosine similarities for all possible pairs of embeddings.\n",
        "\n",
        "        Args:\n",
        "            embeddings (list of torch.Tensor): List of individual speaker embedding tensors.\n",
        "                                               Each tensor should be 1-D, e.g., (embedding_dim,).\n",
        "            speaker_ids_or_filenames (list): List of corresponding speaker IDs (strings)\n",
        "                                             OR filenames (strings) from which IDs can be extracted.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (similarities (np.array), labels (np.array))\n",
        "                   similarities: cosine similarity scores for each pair.\n",
        "                   labels: 1 for same speaker pair, 0 for different speaker pair.\n",
        "        \"\"\"\n",
        "        if not embeddings or len(embeddings) != len(speaker_ids_or_filenames):\n",
        "            raise ValueError(\"Embeddings and speaker_ids_or_filenames lists must be non-empty and of same length.\")\n",
        "\n",
        "        # Determine if we need to extract speaker IDs\n",
        "        # We assume if the first element is a string containing '.wav', it's a filename.\n",
        "        # Otherwise, assume it's already a speaker ID.\n",
        "        if speaker_ids_or_filenames and isinstance(speaker_ids_or_filenames[0], str) and '.wav' in speaker_ids_or_filenames[0]:\n",
        "            actual_speaker_ids = [self._extract_speaker_id_from_filename(f) for f in speaker_ids_or_filenames]\n",
        "            print(\"Note: Speaker IDs extracted from filenames for evaluation.\")\n",
        "        else:\n",
        "            actual_speaker_ids = speaker_ids_or_filenames\n",
        "            print(\"Note: Using provided speaker IDs directly for evaluation.\")\n",
        "\n",
        "\n",
        "        n_embeddings = len(embeddings)\n",
        "        print(f\"Calculating similarities for {n_embeddings} embeddings. This involves \"\n",
        "              f\"{n_embeddings * (n_embeddings - 1) // 2} pairs and can take a while for large datasets.\")\n",
        "\n",
        "        # Stack embeddings into a single tensor for potentially faster batch processing on GPU\n",
        "        # Ensure embeddings are on the correct device\n",
        "        embeddings_tensor = torch.stack(embeddings).to(self.device)\n",
        "\n",
        "        similarities = []\n",
        "        labels = []\n",
        "\n",
        "        # Iterate through all unique pairs (i, j) where i < j\n",
        "        # tqdm provides a progress bar\n",
        "        for i, j in tqdm(itertools.combinations(range(n_embeddings), 2),\n",
        "                         total=n_embeddings * (n_embeddings - 1) // 2,\n",
        "                         desc=\"Calculating embedding pairs\"):\n",
        "            # Cosine similarity expects 2D tensors (batch_size, embedding_dim)\n",
        "            # Unsqueeze adds a batch dimension of 1\n",
        "            sim = F.cosine_similarity(embeddings_tensor[i].unsqueeze(0), embeddings_tensor[j].unsqueeze(0)).item()\n",
        "            similarities.append(sim)\n",
        "            labels.append(1 if actual_speaker_ids[i] == actual_speaker_ids[j] else 0)\n",
        "\n",
        "        return np.array(similarities), np.array(labels)\n",
        "\n",
        "    def _calculate_eer(self, fpr, tpr, thresholds):\n",
        "        \"\"\"\n",
        "        Calculates the Equal Error Rate (EER) and the corresponding threshold.\n",
        "        EER is the point where False Positive Rate (FPR) equals False Negative Rate (FNR).\n",
        "        FNR = 1 - True Positive Rate (TPR).\n",
        "        \"\"\"\n",
        "        fnr = 1 - tpr\n",
        "        # Find the index where FPR and FNR are closest\n",
        "        eer_threshold_idx = np.argmin(np.abs(fnr - fpr))\n",
        "        eer = (fpr[eer_threshold_idx] + fnr[eer_threshold_idx]) / 2 # Average for precision\n",
        "        eer_threshold = thresholds[eer_threshold_idx]\n",
        "        return eer, eer_threshold\n",
        "\n",
        "    def evaluate(self, embeddings, speaker_ids_or_filenames):\n",
        "        \"\"\"\n",
        "        Performs speaker verification evaluation using EER and ROC AUC.\n",
        "\n",
        "        Args:\n",
        "            embeddings (list of torch.Tensor): A list of speaker embedding tensors.\n",
        "                                               Each tensor should be 1-D (embedding_dim,).\n",
        "                                               It's crucial to pass embeddings from a diverse set of\n",
        "                                               speakers and multiple utterances per speaker for\n",
        "                                               meaningful EER/ROC AUC results.\n",
        "            speaker_ids_or_filenames (list): A list of corresponding speaker IDs (strings)\n",
        "                                             OR filenames (strings) for each embedding.\n",
        "                                             If filenames are provided (e.g., 'p229_018.wav'),\n",
        "                                             the speaker ID will be extracted from them.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing:\n",
        "                  'eer': Equal Error Rate.\n",
        "                  'eer_threshold': The threshold at which EER occurs.\n",
        "                  'roc_auc': Area Under the Receiver Operating Characteristic Curve.\n",
        "                  'fpr': False Positive Rates for the ROC curve.\n",
        "                  'tpr': True Positive Rates for the ROC curve.\n",
        "                  'thresholds': Thresholds used for FPR/TPR calculation.\n",
        "        \"\"\"\n",
        "        # Basic input validation\n",
        "        if len(set(speaker_ids_or_filenames)) < 2: # Check unique values in the raw input list\n",
        "            raise ValueError(\"Need embeddings from at least two different speakers for meaningful EER/ROC AUC calculation.\")\n",
        "        if len(embeddings) < 2:\n",
        "            raise ValueError(\"Need at least two embeddings to form pairs for evaluation.\")\n",
        "\n",
        "        # Calculate all pairwise cosine similarities and their true labels\n",
        "        similarities, labels = self._calculate_cosine_similarities(embeddings, speaker_ids_or_filenames)\n",
        "\n",
        "        if len(set(labels)) < 2:\n",
        "            # This happens if all pairs are \"same speaker\" or all are \"different speaker\"\n",
        "            raise ValueError(\"Not enough 'same speaker' AND 'different speaker' pairs for ROC analysis. \"\n",
        "                             \"Ensure your input data contains both types of pairs (i.e., multiple utterances \"\n",
        "                             \"from at least one speaker, and utterances from at least two speakers).\")\n",
        "\n",
        "        # Calculate ROC curve components\n",
        "        fpr, tpr, thresholds = roc_curve(labels, similarities)\n",
        "\n",
        "        # Calculate Area Under Curve (AUC)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Calculate EER and its threshold\n",
        "        eer, eer_threshold = self._calculate_eer(fpr, tpr, thresholds)\n",
        "\n",
        "        results = {\n",
        "            'eer': eer,\n",
        "            'eer_threshold': eer_threshold,\n",
        "            'roc_auc': roc_auc,\n",
        "            'fpr': fpr,\n",
        "            'tpr': tpr,\n",
        "            'thresholds': thresholds\n",
        "        }\n",
        "        return results"
      ],
      "metadata": {
        "id": "6rN2SGNW08Ha"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDutsyndLHcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MRH: MCD metric**"
      ],
      "metadata": {
        "id": "t-_i5V1xLHu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install \"librosa==0.9.1\"\n",
        "!pip install pysptk"
      ],
      "metadata": {
        "id": "E_l1LbLM0i1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pysptk\n",
        "from scipy.spatial.distance import euclidean\n",
        "import librosa # Import librosa for DTW\n",
        "\n",
        "class MCDMetric:\n",
        "    \"\"\"\n",
        "    Calculates Mel-Cepstral Distortion (MCD) and Log-Spectral Distance (LSD)\n",
        "    between two batches of mel-spectrograms, including DTW-based MCD.\n",
        "    Lower MCD/LSD indicates better spectral similarity.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_mfcc: int = 20):\n",
        "        \"\"\"\n",
        "        Initializes the MCDMetric.\n",
        "\n",
        "        Args:\n",
        "            n_mfcc (int): Number of Mel-Frequency Cepstral Coefficients (MFCCs)\n",
        "                          to compute. Typically 12 to 24. Default is 20.\n",
        "        \"\"\"\n",
        "        self.n_mfcc = n_mfcc\n",
        "        print(f\"MCDMetric initialized with n_mfcc={self.n_mfcc}\")\n",
        "\n",
        "    def _mel_to_mcc(self, mel_spec: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Converts a single mel-spectrogram to Mel-Cepstral Coefficients (MCCs).\n",
        "\n",
        "        Args:\n",
        "            mel_spec (np.ndarray): Input mel-spectrogram of shape (n_mels, n_frames).\n",
        "                                   Assumed to be in dB scale (log-power).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: MCCs of shape (n_mfcc, n_frames).\n",
        "        \"\"\"\n",
        "        # Ensure mel_spec is 2D (n_mels, n_frames)\n",
        "        # This handles cases where a single frame or a single mel-bin is passed\n",
        "        if mel_spec.ndim == 1:\n",
        "            mel_spec = mel_spec.reshape(-1, 1) # Treat as single frame if 1D (n_mels,)\n",
        "        elif mel_spec.ndim > 2:\n",
        "            # This shouldn't happen if inputs are correctly batch-indexed, but good for robustness\n",
        "            raise ValueError(f\"mel_spec must be 1D or 2D, but got {mel_spec.ndim}D for _mel_to_mcc.\")\n",
        "\n",
        "        # Handle empty mel_spec to prevent errors in pysptk\n",
        "        if mel_spec.shape[1] == 0: # Check for n_frames == 0\n",
        "            # Return an empty MCC array with correct feature dimension\n",
        "            return np.empty((self.n_mfcc, 0), dtype=mel_spec.dtype)\n",
        "\n",
        "        # Convert from dB to linear power spectrum\n",
        "        # Adding a small epsilon (1e-10) to avoid issues with log(0)\n",
        "        # if the mel_spec has very low (near -inf dB) or zero values.\n",
        "        linear_power_spec = 10**(mel_spec / 10.0) + 1e-10\n",
        "\n",
        "        # Transpose to (n_frames, n_mels) for pysptk, which expects rows as frames\n",
        "        # and columns as spectral bins.\n",
        "        # .copy(order='C') ensures it's C-contiguous, which pysptk prefers.\n",
        "        # pysptk.sptk.mfcc expects a power spectrum and returns MFCCs.\n",
        "        # n_mfcc-1 because the 0th coefficient (energy) is usually included,\n",
        "        # so if you want 20 coefficients, the order is 19.\n",
        "        mccs = pysptk.sptk.mfcc(linear_power_spec.T.copy(order='C'), order=self.n_mfcc - 1)\n",
        "\n",
        "        # Transpose back to (n_mfcc, n_frames) for consistency\n",
        "        return mccs.T\n",
        "\n",
        "    def calculate_mcd(self, mel_spec_batch1: np.ndarray, mel_spec_batch2: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the average frame-by-frame MCD between two batches of mel-spectrograms.\n",
        "\n",
        "        Args:\n",
        "            mel_spec_batch1 (np.ndarray): First batch of mel-spectrograms (e.g., ground truth/target),\n",
        "                                          shape (batch_size, n_mels, n_frames).\n",
        "            mel_spec_batch2 (np.ndarray): Second batch of mel-spectrograms (e.g., model output),\n",
        "                                          shape (batch_size, n_mels, n_frames).\n",
        "\n",
        "        Returns:\n",
        "            float: The average Mel-Cepstral Distortion (MCD) value for the batch.\n",
        "                   Returns 0.0 if no valid MCDs could be calculated or if inputs are invalid.\n",
        "        \"\"\"\n",
        "        # --- Robustness Checks ---\n",
        "        if mel_spec_batch1.shape != mel_spec_batch2.shape:\n",
        "            print(f\"Warning (MCD): Input mel-spectrogram batches have different shapes. \"\n",
        "                  f\"Batch1: {mel_spec_batch1.shape}, Batch2: {mel_spec_batch2.shape}. Cannot calculate MCD.\")\n",
        "            return 0.0\n",
        "        if mel_spec_batch1.ndim != 3 or mel_spec_batch2.ndim != 3:\n",
        "             print(f\"Warning (MCD): Input mel-spectrogram batches must be 3D (batch, mels, frames). \"\n",
        "                  f\"Batch1: {mel_spec_batch1.ndim}D, Batch2: {mel_spec_batch2.ndim}D. Cannot calculate MCD.\")\n",
        "             return 0.0\n",
        "\n",
        "        batch_size = mel_spec_batch1.shape[0]\n",
        "        mcd_values = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            mel_spec1 = mel_spec_batch1[b]\n",
        "            mel_spec2 = mel_spec_batch2[b]\n",
        "\n",
        "            try:\n",
        "                mcc1 = self._mel_to_mcc(mel_spec1)\n",
        "                mcc2 = self._mel_to_mcc(mel_spec2)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting mel-spec to MCC for sample {b}: {e}. Skipping MCD for this sample.\")\n",
        "                continue\n",
        "\n",
        "            min_frames = min(mcc1.shape[1], mcc2.shape[1])\n",
        "            if min_frames == 0:\n",
        "                print(f\"Warning (MCD): No frames to compare for sample {b}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Truncate to min_frames. Ensures both MCCs have same length for element-wise comparison.\n",
        "            mcc1 = mcc1[:, :min_frames]\n",
        "            mcc2 = mcc2[:, :min_frames]\n",
        "\n",
        "            distances = []\n",
        "            for i in range(min_frames):\n",
        "                distances.append(euclidean(mcc1[:, i], mcc2[:, i]))\n",
        "\n",
        "            if distances: # Check if list is not empty before taking mean\n",
        "                mcd_values.append(np.mean(distances))\n",
        "\n",
        "        if not mcd_values:\n",
        "            # This can happen if all samples in batch failed or had 0 frames\n",
        "            return 0.0\n",
        "\n",
        "        return np.mean(mcd_values)\n",
        "\n",
        "    def calculate_mcd_dtw(self, mel_spec_batch1: np.ndarray, mel_spec_batch2: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the average DTW-based MCD between two batches of mel-spectrograms.\n",
        "\n",
        "        Args:\n",
        "            mel_spec_batch1 (np.ndarray): First batch of mel-spectrograms (e.g., ground truth/target),\n",
        "                                          shape (batch_size, n_mels, n_frames).\n",
        "            mel_spec_batch2 (np.ndarray): Second batch of mel-spectrograms (e.g., model output),\n",
        "                                          shape (batch_size, n_mels, n_frames).\n",
        "\n",
        "        Returns:\n",
        "            float: The average DTW-MCD value for the batch.\n",
        "                   Returns 0.0 if no valid MCDs could be calculated or if inputs are invalid.\n",
        "        \"\"\"\n",
        "        # --- Robustness Checks ---\n",
        "        if mel_spec_batch1.shape[0] != mel_spec_batch2.shape[0]:\n",
        "            print(f\"Warning (MCD-DTW): Input mel-spectrogram batches have different batch sizes. \"\n",
        "                  f\"Batch1: {mel_spec_batch1.shape}, Batch2: {mel_spec_batch2.shape}. Cannot calculate MCD-DTW.\")\n",
        "            return 0.0\n",
        "        if mel_spec_batch1.ndim != 3 or mel_spec_batch2.ndim != 3:\n",
        "             print(f\"Warning (MCD-DTW): Input mel-spectrogram batches must be 3D (batch, mels, frames). \"\n",
        "                  f\"Batch1: {mel_spec_batch1.ndim}D, Batch2: {mel_spec_batch2.ndim}D. Cannot calculate MCD-DTW.\")\n",
        "             return 0.0\n",
        "\n",
        "        batch_size = mel_spec_batch1.shape[0]\n",
        "        mcd_dtw_values = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            mel_spec1 = mel_spec_batch1[b]\n",
        "            mel_spec2 = mel_spec_batch2[b]\n",
        "\n",
        "            try:\n",
        "                mcc1 = self._mel_to_mcc(mel_spec1)\n",
        "                mcc2 = self._mel_to_mcc(mel_spec2)\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting mel-spec to MCC for sample {b}: {e}. Skipping MCD-DTW for this sample.\")\n",
        "                continue\n",
        "\n",
        "            # Ensure there are enough frames for meaningful DTW\n",
        "            # librosa.sequence.dtw will raise an error if input sequences are too short (e.g., 0 or 1 frame)\n",
        "            # A common heuristic is to require at least 2 frames for DTW.\n",
        "            if mcc1.shape[1] < 2 or mcc2.shape[1] < 2:\n",
        "                print(f\"Warning (MCD-DTW): Not enough frames for DTW for sample {b} (MCC1: {mcc1.shape[1]}, MCC2: {mcc2.shape[1]}). Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Ensure MCCs are C-contiguous for librosa.dtw, it expects (features, frames)\n",
        "            # .ascontiguousarray() creates a copy if not already contiguous.\n",
        "            mcc1_dtw = np.ascontiguousarray(mcc1)\n",
        "            mcc2_dtw = np.ascontiguousarray(mcc2)\n",
        "\n",
        "            try:\n",
        "                # D is the accumulated cost matrix, wp is the warping path\n",
        "                # metric='euclidean' is the default for librosa.sequence.dtw and appropriate for MCCs.\n",
        "                D, wp = librosa.sequence.dtw(X=mcc1_dtw, Y=mcc2_dtw, metric='euclidean')\n",
        "\n",
        "                # The `wp` (warping path) can sometimes be empty if `librosa.sequence.dtw`\n",
        "                # encounters very problematic inputs (e.g., extremely short, or if max_inst/max_hop\n",
        "                # constraints make it impossible to find a path, though default should be fine).\n",
        "                # Check if wp is not empty to avoid division by zero or errors from indexing it.\n",
        "                if wp.shape[0] == 0:\n",
        "                    print(f\"Warning (MCD-DTW): DTW warping path is empty for sample {b}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Calculate the average distance along the optimal warping path.\n",
        "                # Loop through the aligned frame pairs (r, c) from the warping path `wp`.\n",
        "                # For each pair, calculate the Euclidean distance between the corresponding MCC vectors.\n",
        "                path_distances = []\n",
        "                for r, c in wp: # wp contains (row_idx_X, col_idx_Y) pairs for aligned points\n",
        "                    path_distances.append(euclidean(mcc1_dtw[:, r], mcc2_dtw[:, c]))\n",
        "\n",
        "                if path_distances: # Ensure there are distances before taking mean\n",
        "                    mcd_dtw_values.append(np.mean(path_distances))\n",
        "                else:\n",
        "                    # This case should ideally be caught by wp.shape[0] == 0 check, but a fallback.\n",
        "                    print(f\"Warning (MCD-DTW): Calculated path_distances list is empty for sample {b}. Skipping.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating DTW for sample {b}: {e}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "        if not mcd_dtw_values:\n",
        "            # This can happen if all samples in batch failed or had insufficient frames\n",
        "            print(\"Warning: No valid MCD-DTW values calculated for the entire batch. Returning 0.0.\")\n",
        "            return 0.0\n",
        "\n",
        "        return np.mean(mcd_dtw_values)\n",
        "\n",
        "    def calculate_lsd(self, mel_spec_batch1: np.ndarray, mel_spec_batch2: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calculates the average Log-Spectral Distance (LSD) between two batches of mel-spectrograms.\n",
        "\n",
        "        Args:\n",
        "            mel_spec_batch1 (np.ndarray): First batch of mel-spectrograms (e.g., ground truth/target),\n",
        "                                          shape (batch_size, n_mels, n_frames).\n",
        "            mel_spec_batch2 (np.ndarray): Second batch of mel-spectrograms (e.g., model output),\n",
        "                                          shape (batch_size, n_mels, n_frames).\n",
        "\n",
        "        Returns:\n",
        "            float: The average Log-Spectral Distance (LSD) value for the batch.\n",
        "                   Returns 0.0 if no valid LSDs could be calculated or if inputs are invalid.\n",
        "        \"\"\"\n",
        "        # --- Robustness Checks ---\n",
        "        if mel_spec_batch1.shape != mel_spec_batch2.shape:\n",
        "            print(f\"Warning (LSD): Input mel-spectrogram batches have different shapes. \"\n",
        "                  f\"Batch1: {mel_spec_batch1.shape}, Batch2: {mel_spec_batch2.shape}. Cannot calculate LSD.\")\n",
        "            return 0.0\n",
        "        if mel_spec_batch1.ndim != 3 or mel_spec_batch2.ndim != 3:\n",
        "             print(f\"Warning (LSD): Input mel-spectrogram batches must be 3D (batch, mels, frames). \"\n",
        "                  f\"Batch1: {mel_spec_batch1.ndim}D, Batch2: {mel_spec_batch2.ndim}D. Cannot calculate LSD.\")\n",
        "             return 0.0\n",
        "\n",
        "        batch_size = mel_spec_batch1.shape[0]\n",
        "        lsd_values = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            mel_spec1 = mel_spec_batch1[b]\n",
        "            mel_spec2 = mel_spec_batch2[b]\n",
        "\n",
        "            # Convert to (n_mels, n_frames) if needed (already handled by batch indexing)\n",
        "            # Ensure inputs are treated as log-power spectra (dB scale).\n",
        "            # If your mel_spec is already in dB, no explicit conversion needed here.\n",
        "            # Assuming mel_spec_batch1 and mel_spec_batch2 are already in dB scale.\n",
        "            log_mel_spec1 = mel_spec1\n",
        "            log_mel_spec2 = mel_spec2\n",
        "\n",
        "            # Handle empty mel_spec to prevent errors\n",
        "            if log_mel_spec1.shape[1] == 0 or log_mel_spec2.shape[1] == 0:\n",
        "                print(f\"Warning (LSD): No frames to compare for sample {b}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Ensure both log-mel-spectrograms have the same number of frames\n",
        "                min_frames = min(log_mel_spec1.shape[1], log_mel_spec2.shape[1])\n",
        "\n",
        "                # If there are no common frames, skip this sample\n",
        "                if min_frames == 0:\n",
        "                    print(f\"Warning (LSD): No common frames for sample {b}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                truncated_log_mel_spec1 = log_mel_spec1[:, :min_frames]\n",
        "                truncated_log_mel_spec2 = log_mel_spec2[:, :min_frames]\n",
        "\n",
        "                # Calculate the difference between the two log-mel-spectrograms\n",
        "                diff = truncated_log_mel_spec1 - truncated_log_mel_spec2\n",
        "\n",
        "                # Square the differences\n",
        "                squared_diff = np.square(diff)\n",
        "\n",
        "                # Sum over frequency bins (axis=0 for n_mels) for each frame\n",
        "                sum_squared_diff_per_frame = np.sum(squared_diff, axis=0)\n",
        "\n",
        "                # Take the square root of the sum_squared_diff_per_frame (this is the Euclidean distance per frame)\n",
        "                # Then average over frames to get the LSD for this sample\n",
        "                sample_lsd = np.mean(np.sqrt(sum_squared_diff_per_frame))\n",
        "                lsd_values.append(sample_lsd)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating LSD for sample {b}: {e}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "        if not lsd_values:\n",
        "            print(\"Warning: No valid LSD values calculated for the entire batch. Returning 0.0.\")\n",
        "            return 0.0\n",
        "\n",
        "        return np.mean(lsd_values)"
      ],
      "metadata": {
        "id": "__ef6q1QZZsj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MRH: Some Changes to class solver to include Speaker Verification**"
      ],
      "metadata": {
        "id": "cUh9ohre3Gwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adaptive_voice_conversion /solver.py\n",
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import yaml\n",
        "import pickle\n",
        "# from model import AE\n",
        "# from data_utils import get_data_loader\n",
        "# from data_utils import PickleDataset\n",
        "# from utils import *\n",
        "from functools import reduce\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm  ############### > MRH: change\n",
        "\n",
        "class Solver(object):\n",
        "    def __init__(self, config, args):\n",
        "        # print('MRH1')\n",
        "        # config store the value of hyperparameters, turn to attr by AttrDict\n",
        "        self.config = config\n",
        "        # print(config)\n",
        "        # print('MRH2')\n",
        "\n",
        "        # args store other information\n",
        "        self.args = args\n",
        "        # print(self.args)\n",
        "        # print(' self.args.use_content_hierarchy = ',self.args.use_content_hierarchy)\n",
        "        # print('MRH3')\n",
        "\n",
        "        # logger to use tensorboard\n",
        "        self.logger = Logger(self.args.logdir)\n",
        "        # print('MRH4')\n",
        "\n",
        "        # get dataloader\n",
        "        self.get_data_loaders()\n",
        "        # print('MRH5')\n",
        "\n",
        "        # init the model with config\n",
        "        self.build_model()\n",
        "        # print('MRH6')\n",
        "        self.save_config()\n",
        "        # print('MRH7')\n",
        "\n",
        "        if args.load_model:\n",
        "            self.load_model()\n",
        "\n",
        "        # print('MRH8')\n",
        "\n",
        "        ############### MRH: Loss\n",
        "        ae1=AE1(self.args)\n",
        "        self.ae_loss_calculator = AELoss(n_speaker_levels=ae1.n_speaker_levels,\n",
        "                                         n_content_levels=ae1.n_content_levels)\n",
        "\n",
        "\n",
        "        ####### MRH: This is the change\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "        ####\n",
        "        # self.speaker_verifier = SpeakerVerificationEvaluator(device=self.args.device if hasattr(self.args, 'device') else 'cpu')\n",
        "        self.speaker_verifier = SpeakerVerificationEvaluator(device)\n",
        "\n",
        "\n",
        "        #################################### MRH: MCD\n",
        "        # Initialize the MCD metric\n",
        "        self.mcd_metric = MCDMetric(n_mfcc=20)\n",
        "        # List to store MCD values for plotting\n",
        "        self.mcd_values_history = [] # <-- ADD THIS LINE\n",
        "\n",
        "    def save_model(self, iteration):\n",
        "        # save model and discriminator and their optimizer\n",
        "        torch.save(self.model.state_dict(), f'{self.args.store_model_path}.ckpt')\n",
        "        torch.save(self.opt.state_dict(), f'{self.args.store_model_path}.opt')\n",
        "\n",
        "    def save_config(self):\n",
        "        with open(f'{self.args.store_model_path}.config.yaml', 'w') as f:\n",
        "            yaml.dump(self.config, f)\n",
        "        with open(f'{self.args.store_model_path}.args.yaml', 'w') as f:\n",
        "            yaml.dump(vars(self.args), f)\n",
        "        return\n",
        "\n",
        "    def load_model(self):\n",
        "        print(f'Load model from {self.args.load_model_path}')\n",
        "        self.model.load_state_dict(torch.load(f'{self.args.load_model_path}.ckpt'))\n",
        "        self.opt.load_state_dict(torch.load(f'{self.args.load_model_path}.opt'))\n",
        "        return\n",
        "\n",
        "    def get_data_loaders(self):\n",
        "        data_dir = self.args.data_dir\n",
        "        self.train_dataset = PickleDataset(os.path.join(data_dir, f'{self.args.train_set}.pkl'),\n",
        "                os.path.join(data_dir, self.args.train_index_file),\n",
        "                segment_size=self.config['data_loader']['segment_size'])\n",
        "        self.train_loader = get_data_loader(self.train_dataset,\n",
        "                frame_size=self.config['data_loader']['frame_size'],\n",
        "                batch_size=self.config['data_loader']['batch_size'],\n",
        "                shuffle=self.config['data_loader']['shuffle'],\n",
        "                num_workers=4, drop_last=False)\n",
        "        self.train_iter = infinite_iter(self.train_loader)\n",
        "\n",
        "\n",
        "        ############################################ MRH: change\n",
        "\n",
        "        # This is highly recommended for proper evaluation.\n",
        "        # You would need to ensure your PickleDataset and get_data_loader\n",
        "        # correctly yield (audio_features, speaker_id_or_filename)\n",
        "\n",
        "        # self.val_dataset = PickleDataset(os.path.join(data_dir, f'{self.args.val_set}.pkl'),\n",
        "        #                                  os.path.join(data_dir, self.args.val_index_file),\n",
        "        #                                  segment_size=self.config['data_loader']['segment_size'])\n",
        "        # self.val_loader = get_data_loader(self.val_dataset,\n",
        "        #                                   frame_size=self.config['data_loader']['frame_size'],\n",
        "        #                                   batch_size=self.config['data_loader']['batch_size'],\n",
        "        #                                   shuffle=False, # No need to shuffle validation data\n",
        "        #                                   num_workers=4, drop_last=False)\n",
        "        # self.val_iter = infinite_iter(self.val_loader) # If you want to iterate infinitely\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "    def build_model(self):\n",
        "        # create model, discriminator, optimizers\n",
        "        # print('---')\n",
        "        # self.model = cc(AE1(self.config))\n",
        "        self.model = cc(AE1(self.args))\n",
        "        # print('***')\n",
        "        # print(self.model)\n",
        "        optimizer = self.config['optimizer']\n",
        "        self.opt = torch.optim.Adam(self.model.parameters(),\n",
        "                lr=optimizer['lr'], betas=(optimizer['beta1'], optimizer['beta2']),\n",
        "                amsgrad=optimizer['amsgrad'], weight_decay=optimizer['weight_decay'])\n",
        "        # print(self.opt)\n",
        "        return\n",
        "    ########################################################\n",
        "    # Assuming 'model' is an instance of AE and 'batch' contains input 'x'\n",
        "    # This should be part of your training loop\n",
        "    # def ae_step(model, x):\n",
        "    def ae_step(self,x,lambda_kl):\n",
        "        x = cc(x)\n",
        "        output = self.model(x)\n",
        "        dec = output['dec']\n",
        "        speaker_latents = output['speaker_latents']\n",
        "        content_latents = output['content_latents']\n",
        "\n",
        "        # # Reconstruction Loss (e.g., L1, MSE)\n",
        "        # recon_loss = F.l1_loss(dec, x) # Or F.mse_loss(dec, x)\n",
        "\n",
        "        # # KLD Loss for Speaker Hierarchy\n",
        "        # speaker_kld_loss = 0\n",
        "        # # Loop for each level (0 for z1, 1 for z2, etc.)\n",
        "        # for i in range(self.model.n_speaker_levels):\n",
        "        #     mu_post = speaker_latents['mus'][i]\n",
        "        #     log_sigma_post = speaker_latents['log_sigmas'][i]\n",
        "\n",
        "        #     # For z1 (i=0), mu_prior and log_sigma_prior will be None (standard normal)\n",
        "        #     # For z2 (i=1), mu_prior and log_sigma_prior will be conditioned on z1\n",
        "        #     # For z3 (i=2), mu_prior and log_sigma_prior will be conditioned on z2\n",
        "        #     mu_prior = speaker_latents['mu_priors'][i]\n",
        "        #     log_sigma_prior = speaker_latents['log_sigma_priors'][i]\n",
        "\n",
        "        #     speaker_kld_loss += calculate_kld(mu_post, log_sigma_post, mu_prior, log_sigma_prior)\n",
        "\n",
        "        # # KLD Loss for Content Hierarchy\n",
        "        # content_kld_loss = 0\n",
        "        # # Loop for each level (0 for z1, 1 for z2, etc.)\n",
        "        # for i in range(self.model.n_content_levels):\n",
        "        #     mu_post = content_latents['mus'][i]\n",
        "        #     log_sigma_post = content_latents['log_sigmas'][i]\n",
        "\n",
        "        #     # For z1 (i=0), mu_prior and log_sigma_prior will be None (standard normal)\n",
        "        #     # For z2 (i=1), mu_prior and log_sigma_prior will be conditioned on z1\n",
        "        #     # For z3 (i=2), mu_prior and log_sigma_prior will be conditioned on z2\n",
        "        #     mu_prior = content_latents['mu_priors'][i]\n",
        "        #     log_sigma_prior = content_latents['log_sigma_priors'][i]\n",
        "\n",
        "        #     content_kld_loss += calculate_kld(mu_post, log_sigma_post, mu_prior, log_sigma_prior)\n",
        "\n",
        "        # # Total Loss\n",
        "        # # You might want to weigh KLD losses\n",
        "        # total_loss = recon_loss + speaker_kld_loss + content_kld_loss\n",
        "\n",
        "        # return total_loss, recon_loss, speaker_kld_loss, content_kld_loss\n",
        "\n",
        "        loss_components = self.ae_loss_calculator.loss_calculate(x, dec, speaker_latents, content_latents, lambda_kl)\n",
        "        loss= loss_components['total_loss']\n",
        "\n",
        "        ###\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
        "                max_norm=self.config['optimizer']['grad_norm'])\n",
        "        self.opt.step()\n",
        "\n",
        "        ###\n",
        "\n",
        "        return loss_components, output\n",
        "\n",
        "        # return {\n",
        "        #     'recon_loss': recon_loss,\n",
        "        #     'speaker_kld_loss': speaker_kld_loss,\n",
        "        #     'content_kld_loss': content_kld_loss\n",
        "        #     # You might add other relevant values here if they are directly calculated\n",
        "        #     # and consistently available within ae_step (e.g., total_loss and grad_norm\n",
        "        #     # if ae_step also handled optimization, but we decided against that earlier).\n",
        "        # }\n",
        "################################################################################\n",
        "    def train(self, n_iterations):\n",
        "        # print('mrh 1')\n",
        "        batch_mcd = 0.0 # Initialize with a default value\n",
        "        batch_mcd_dtw = 0.0\n",
        "        batch_lsd=0.0\n",
        "        for iteration in range(n_iterations):\n",
        "            # print('mrh 2')\n",
        "            # print('iteration= ',iteration)\n",
        "            if iteration >= self.config['annealing_iters']:\n",
        "                lambda_kl = self.config['lambda']['lambda_kl']\n",
        "            else:\n",
        "                lambda_kl = self.config['lambda']['lambda_kl'] * (iteration + 1) / self.config['annealing_iters']\n",
        "            data = next(self.train_iter)\n",
        "            # print('mrh 3')\n",
        "            # print('data= ',data)\n",
        "            # print('\\ndata length= ',len(data))\n",
        "            # print('\\ndata type= ',type(data))\n",
        "            # print('\\ndata[0].shape=  ', data[0].shape)\n",
        "            # print('\\nlen(data[1])=  ', len(data[1]))\n",
        "\n",
        "            # raise Exception('Mohammad Reza Hasanabadi')\n",
        "\n",
        "            ######################################################### MRH: change\n",
        "            # meta = self.ae_step(data, lambda_kl)\n",
        "            meta , output= self.ae_step(data[0] if isinstance(data, (list, tuple)) else data, lambda_kl)\n",
        "\n",
        "\n",
        "            # print('meta type = ',type(meta))\n",
        "            # print('mrh 4')\n",
        "            # add to logger\n",
        "            if iteration % self.args.summary_steps == 0:\n",
        "                # print('mrh 5')\n",
        "                self.logger.scalars_summary(f'{self.args.tag}/ae_train', meta, iteration)\n",
        "                # print('mrh 6')\n",
        "            # loss_rec = meta['loss_rec']\n",
        "            loss_rec = meta['recon_loss']\n",
        "            # print('mrh 7')\n",
        "            # loss_kl = meta['loss_kl']\n",
        "            speaker_loss_kl = meta['speaker_kld_loss']\n",
        "            content_loss_kl = meta['content_kld_loss']\n",
        "            # print('mrh 8')\n",
        "\n",
        "            # print(f'AE:[{iteration + 1}/{n_iterations}], loss_rec={loss_rec:.2f}, '\n",
        "                    # f'loss_kl={loss_kl:.2f}, lambda={lambda_kl:.1e}     ', end='\\r')\n",
        "\n",
        "\n",
        "            if (iteration + 1) % self.args.save_steps == 0 or iteration + 1 == n_iterations:\n",
        "                self.save_model(iteration=iteration)\n",
        "                print()\n",
        "\n",
        "            ############################################################################# MRH: change\n",
        "\n",
        "            if (iteration + 1) % 10 == 0 :\n",
        "                self.model.eval() # Still good practice to put model in eval mode for this\n",
        "\n",
        "                # print('1')\n",
        "                audio_data_batch = data[0] # Use the current batch's audio data\n",
        "                speaker_ids_batch = data[1]                 # Use the current batch's speaker IDs\n",
        "\n",
        "                all_embeddings = []\n",
        "                all_speaker_ids = []\n",
        "\n",
        "                # print('2')\n",
        "                # with torch.no_grad():\n",
        "                audio_data_batch = cc(audio_data_batch)\n",
        "                # print('audio_data_batch shape =',audio_data_batch.shape)\n",
        "                model_output = self.model(audio_data_batch)\n",
        "                # print('22')\n",
        "                embeddings_batch = model_output['speaker_latents']['decoder_input']\n",
        "\n",
        "                # Collect individual embeddings and speaker IDs from the current batch\n",
        "                print('3')\n",
        "                for i in range(embeddings_batch.size(0)):\n",
        "                    all_embeddings.append(embeddings_batch[i].cpu())\n",
        "                    all_speaker_ids.append(speaker_ids_batch[i])\n",
        "\n",
        "                print('4')\n",
        "                try:\n",
        "                    # Check if there are enough distinct speakers for a meaningful EER\n",
        "                    if len(set(all_speaker_ids)) < 2 or len(all_embeddings) < 2:\n",
        "                        print(\"Skipping batch-level speaker verification: Not enough unique speakers or samples in batch.\")\n",
        "                    else:\n",
        "                        eval_results = self.speaker_verifier.evaluate(all_embeddings, all_speaker_ids)\n",
        "                        # Log with a different tag to distinguish from full eval\n",
        "                        self.logger.scalars_summary(f'{self.args.tag}/speaker_verification_batch', {\n",
        "                            'eer': eval_results['eer'],\n",
        "                            'roc_auc': eval_results['roc_auc'],\n",
        "                            'eer_threshold': eval_results['eer_threshold']\n",
        "                        }, iteration)\n",
        "                        print(f\"Batch Speaker Verification Results: EER={eval_results['eer']:.4f}, ROC AUC={eval_results['roc_auc']:.4f}\")\n",
        "                except ValueError as e:\n",
        "                    print(f\"Skipping batch-level speaker verification due to data inadequacy: {e}\")\n",
        "\n",
        "                self.model.train()\n",
        "                print(\"-----------------------------------------------------------------------------------\\n\")\n",
        "            # --- End Speaker Verification on Current Batch ---\n",
        "\n",
        "\n",
        "\n",
        "            ###################################################### MRH: MCD\n",
        "                        # Add to logger and calculate MCD only when summary_steps condition is met\n",
        "            if iteration % 5 == 0:\n",
        "                # self.logger.scalars_summary(f'{self.args.tag}/ae_train', meta, iteration)\n",
        "\n",
        "                # --- Start: MCDMetric Calculation Moved Inside Summary Block ---\n",
        "\n",
        "                # Convert PyTorch tensors to NumPy arrays for metric calculation\n",
        "                # Ensure they are on CPU and detached from graph\n",
        "                source_mels = data[0].cpu().detach().numpy()\n",
        "                converted_mels = output['dec'].cpu().detach().numpy()\n",
        "\n",
        "                # Calculate MCD for the current batch using the initialized mcd_metric instance\n",
        "                batch_mcd = self.mcd_metric.calculate_mcd(source_mels, converted_mels)\n",
        "\n",
        "\n",
        "                # Calculate MCD-DTW\n",
        "                batch_mcd_dtw = self.mcd_metric.calculate_mcd_dtw(source_mels, converted_mels)\n",
        "\n",
        "                # Calculate lsd\n",
        "                batch_lsd = self.mcd_metric.calculate_lsd(source_mels, converted_mels)\n",
        "\n",
        "                # Log MCD\n",
        "                # self.logger.scalars_summary(f'{self.args.tag}/metrics/mcd', {'mcd': batch_mcd}, iteration)\n",
        "\n",
        "                # Store the calculated MCD value\n",
        "                self.mcd_values_history.append(batch_mcd) # <-- STORE HERE\n",
        "\n",
        "                # Print the average MCD immediately\n",
        "                # print(f\"MCD calculated at iteration {iteration}: {batch_mcd:.4f}\") # <-- PRINT HERE\n",
        "\n",
        "                # --- End: MCDMetric Calculation Moved Inside Summary Block ---\n",
        "\n",
        "\n",
        "            ####################################\n",
        "\n",
        "            print(f'AE:[{iteration + 1}/{n_iterations}], loss_rec={meta[\"recon_loss\"]:.2f}, '\n",
        "                    f'speaker_loss_kl={meta[\"speaker_kld_loss\"]:.2f}, content_loss_kl={meta[\"content_kld_loss\"]:.2f} ,'\n",
        "                    f'lambda={lambda_kl:.1e}, MCD={batch_mcd:.4f}, MCD_DTW={batch_mcd_dtw:.4f}, batch_lsd={batch_lsd:.4f}') # This will print 0.0 if not calculated in this iter.\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "cJvMSEfw1C4t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import ArgumentParser, Namespace\n",
        "import torch\n",
        "# from solver import Solver # Make sure Solver is defined or imported\n",
        "import yaml\n",
        "import sys\n",
        "\n",
        "# Assume your ARGS class (with the embedded config) is defined above or imported\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args1 = ARGS1() # Instantiate your ARGS class\n",
        "\n",
        "    # The config is already loaded into args.config within the ARGS __init__\n",
        "    # So, you just need to assign it to a local variable 'config' if you want\n",
        "    config = args1.config\n",
        "\n",
        "    # Now, pass this 'config' dictionary to your Solver\n",
        "    solver = Solver(config=config, args=args1)\n",
        "\n",
        "    if args1.iters > 0:\n",
        "        solver.train(n_iterations=args1.iters)"
      ],
      "metadata": {
        "id": "veK_Xykt1Dpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}